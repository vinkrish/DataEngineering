{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"About"},{"location":"big_data/","text":"Big Data Hadoop Hadoop is an open source software platform for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware. Ambari Ambari is an Apache project supported by Hortonworks. It offers a web based GUI (Graphical User Interface) with wizard scripts for setting up clusters with most of the standard components. Ambari provisions, manages and monitors all the clusters of Hadoop jobs. MapReduce MapReduce distributes the processing of data on your cluster. Divides your data up intro partitions that are MAPPED (transformed) and REDUCED (aggregated) by mapper and reducer functions you define. MapReduce is a programming model and an associated implementation for processing and generating big data sets with a parallel, distributed algorithm on a cluster. A MapReduce program is composed of a map procedure (or method) - which performs filtering and sorting (such as sorting students by first name into queues, one queue for each name), and a reduce method - which performs a summary operation (such as counting the number of students in each queue, yielding name frequencies). The \"MapReduce System\" (also called \"infrastructure\" or \"framework\") orchestrates the processing by marshalling the distributed servers, running the various tasks in parallel, managing all communications and data transfers between the various parts of the system, and providing for redundancy and fault tolerance. In computer science, marshalling is the process of transforming the memory representation of an object to a data format suitable for storage or transmission, and it is typically used when data must be moved between different parts of a computer program or from one program to another. Marshalling is similar to serialization and is used to communicate to remote objects with an object, in this case a serialized object. HDFS (Hadoop Distributed File System) The HDFS, distributed under Apache license offers a basic framework for splitting up data collections between multiple nodes. In HDFS, the large files are broken into blocks, where several nodes hold all of the blocks from a file. The file system is designed in a way to mix fault tolerance with high throughput. The blocks of HDFS are loaded to maintain steady streaming. They are not usually cached to minimize latency. Pig When the data stored is visible to Hadoop, Apache Pig dives into the data and runs the code that is written in its own language, called Pig Latin. Pig Latin is filled with abstractions for handling the data. Pig comes with standard functions for common tasks like averaging data, working with dates, or to find differences between strings. Pig also allows the user to write languages of their own, called UDF (User Defined Function), when the standard functions fall short. Hive If you are already fluent with SQL, then you can leverage Hadoop using Hive. Hive was developed by some folks at Facebook. Apache Hive regulates the process of extracting bits from all the files in HBase. It supports analysis of large datasets stored in Hadoop\u2019s HDFS and compatible file systems. It also provides an SQL like language called HSQL (HiveSQL) that gets into the files and extracts the required snippets for the code. TEZ A Framework for YARN-based, Data Processing Applications in Hadoop. Apache Tez is an extensible framework for building high performance batch and interactive data processing applications, coordinated by YARN in Apache Hadoop. Tez improves the MapReduce paradigm by dramatically improving its speed, while maintaining MapReduce\u2019s ability to scale to petabytes of data. Important Hadoop ecosystem projects like Apache Hive and Apache Pig use Apache Tez. Spark Spark is the next generation that pretty much works like Hadoop that processes data cached in the memory. Its objective is to make data analysis fast to run and write with a general execution model. This can optimize arbitrary operator graphs and support in-memory computing, which lets it query data faster than disk-based engines like Hadoop. It also supports streaming data, run ML algorithms on clusters. HBase HBase is a column-oriented database management system that runs on top of HDFS. HBase applications are written in Java, very much like the MapReduce application. It comprises a set of tables, where each table contains rows and columns like a traditional database. When the data falls into the big table, HBase will store the data, search it and automatically share the table across multiple nodes so that MapReduce jobs can run it locally. HBase offers a limited guarantee for some local changes. The changes that happen in a single row can succeed or fail at the same time. It has a dynamic data model and not a relational DBMS. Storm A system for processing streaming data in real time. Apache Storm adds reliable real-time data processing capabilities to Enterprise Hadoop. Storm on YARN is powerful for scenarios requiring real-time analytics, machine learning and continuous monitoring of operations. NoSQL Some Hadoop clusters integrate with NoSQL data stores that come with their own mechanisms for storing data across a cluster of nodes. This allows them to store and retrieve data with all the features of the NoSQL database, after which Hadoop can be used to schedule data analysis jobs on the same cluster. Mahout Mahout is designed to implement a great number of algorithms, classifications and filtering of data analysis to Hadoop cluster. Many of the standard algorithms like K-means, Dirichelet, parallel pattern and Bayesian classifications are ready to run on the data with a Hadoop style Map and reduce. Lucene/Solr Lucene, written in Java integrates easily with Hadoop and is a natural companion for Hadoop. It is a tool meant for indexing large blocks of unstructured text. Lucene handles the indexing, while Hadoop handles the distributed queries across the cluster. Lucene-Hadoop features are rapidly evolving as new projects are being developed. Avro Avro is a serialization system that bundles the data together with a schema for understanding it. Each packet comes with a JSON data structure. JSON explains how the data can be parsed. The header of JSON specifies the structure for the data, where the need to write extra tags in the data to mark the fields can be avoided. The output is considerably more compact than the traditional formats like XML. Oozie A job can be simplified by breaking it into steps. On breaking the project in to multiple Hadoop jobs, Oozie starts processing them in the right sequence. It manages the workflow as specified by DAG (Directed Acyclic Graph) and there is no need for timely monitor. Sqoop Apache Sqoop is specially designed to transfer bulk data efficiently from the traditional databases into Hive or HBase. It can also be used to extract data from Hadoop and export it to external structured data-stores like relational databases and enterprise data warehouses. Sqoop is a command line tool, mapping between the tables and the data storage layer, translating the tables into a configurable combination of HDFS, HBase or Hive. Flume Gathering all the data is equal to storing and analyzing it. Apache Flume dispatches \u2018special agents\u2019 to gather information that will be stored in HDFS. The information gathered can be log files, Twitter API, or website scraps. These data can be chained and subjected to analyses. Kafka Apache Kafka is an open-source stream-processing software. It aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Its storage layer is essentially a \"massively scalable pub/sub message queue designed as a distributed transaction log\". Zookeeper Zookeeper is a centralized service that maintains, configures information, gives a name and provides distributed synchronization across a cluster. It imposes a file system-like hierarchy on the cluster and stores all of the metadata for the machines, so we can synchronize the work of the various machines.","title":"Big Data"},{"location":"big_data/#big-data","text":"","title":"Big Data"},{"location":"big_data/#hadoop","text":"Hadoop is an open source software platform for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware.","title":"Hadoop"},{"location":"big_data/#ambari","text":"Ambari is an Apache project supported by Hortonworks. It offers a web based GUI (Graphical User Interface) with wizard scripts for setting up clusters with most of the standard components. Ambari provisions, manages and monitors all the clusters of Hadoop jobs.","title":"Ambari"},{"location":"big_data/#mapreduce","text":"MapReduce distributes the processing of data on your cluster. Divides your data up intro partitions that are MAPPED (transformed) and REDUCED (aggregated) by mapper and reducer functions you define. MapReduce is a programming model and an associated implementation for processing and generating big data sets with a parallel, distributed algorithm on a cluster. A MapReduce program is composed of a map procedure (or method) - which performs filtering and sorting (such as sorting students by first name into queues, one queue for each name), and a reduce method - which performs a summary operation (such as counting the number of students in each queue, yielding name frequencies). The \"MapReduce System\" (also called \"infrastructure\" or \"framework\") orchestrates the processing by marshalling the distributed servers, running the various tasks in parallel, managing all communications and data transfers between the various parts of the system, and providing for redundancy and fault tolerance. In computer science, marshalling is the process of transforming the memory representation of an object to a data format suitable for storage or transmission, and it is typically used when data must be moved between different parts of a computer program or from one program to another. Marshalling is similar to serialization and is used to communicate to remote objects with an object, in this case a serialized object.","title":"MapReduce"},{"location":"big_data/#hdfs-hadoop-distributed-file-system","text":"The HDFS, distributed under Apache license offers a basic framework for splitting up data collections between multiple nodes. In HDFS, the large files are broken into blocks, where several nodes hold all of the blocks from a file. The file system is designed in a way to mix fault tolerance with high throughput. The blocks of HDFS are loaded to maintain steady streaming. They are not usually cached to minimize latency.","title":"HDFS (Hadoop Distributed File System)"},{"location":"big_data/#pig","text":"When the data stored is visible to Hadoop, Apache Pig dives into the data and runs the code that is written in its own language, called Pig Latin. Pig Latin is filled with abstractions for handling the data. Pig comes with standard functions for common tasks like averaging data, working with dates, or to find differences between strings. Pig also allows the user to write languages of their own, called UDF (User Defined Function), when the standard functions fall short.","title":"Pig"},{"location":"big_data/#hive","text":"If you are already fluent with SQL, then you can leverage Hadoop using Hive. Hive was developed by some folks at Facebook. Apache Hive regulates the process of extracting bits from all the files in HBase. It supports analysis of large datasets stored in Hadoop\u2019s HDFS and compatible file systems. It also provides an SQL like language called HSQL (HiveSQL) that gets into the files and extracts the required snippets for the code.","title":"Hive"},{"location":"big_data/#tez","text":"A Framework for YARN-based, Data Processing Applications in Hadoop. Apache Tez is an extensible framework for building high performance batch and interactive data processing applications, coordinated by YARN in Apache Hadoop. Tez improves the MapReduce paradigm by dramatically improving its speed, while maintaining MapReduce\u2019s ability to scale to petabytes of data. Important Hadoop ecosystem projects like Apache Hive and Apache Pig use Apache Tez.","title":"TEZ"},{"location":"big_data/#spark","text":"Spark is the next generation that pretty much works like Hadoop that processes data cached in the memory. Its objective is to make data analysis fast to run and write with a general execution model. This can optimize arbitrary operator graphs and support in-memory computing, which lets it query data faster than disk-based engines like Hadoop. It also supports streaming data, run ML algorithms on clusters.","title":"Spark"},{"location":"big_data/#hbase","text":"HBase is a column-oriented database management system that runs on top of HDFS. HBase applications are written in Java, very much like the MapReduce application. It comprises a set of tables, where each table contains rows and columns like a traditional database. When the data falls into the big table, HBase will store the data, search it and automatically share the table across multiple nodes so that MapReduce jobs can run it locally. HBase offers a limited guarantee for some local changes. The changes that happen in a single row can succeed or fail at the same time. It has a dynamic data model and not a relational DBMS.","title":"HBase"},{"location":"big_data/#storm","text":"A system for processing streaming data in real time. Apache Storm adds reliable real-time data processing capabilities to Enterprise Hadoop. Storm on YARN is powerful for scenarios requiring real-time analytics, machine learning and continuous monitoring of operations.","title":"Storm"},{"location":"big_data/#nosql","text":"Some Hadoop clusters integrate with NoSQL data stores that come with their own mechanisms for storing data across a cluster of nodes. This allows them to store and retrieve data with all the features of the NoSQL database, after which Hadoop can be used to schedule data analysis jobs on the same cluster.","title":"NoSQL"},{"location":"big_data/#mahout","text":"Mahout is designed to implement a great number of algorithms, classifications and filtering of data analysis to Hadoop cluster. Many of the standard algorithms like K-means, Dirichelet, parallel pattern and Bayesian classifications are ready to run on the data with a Hadoop style Map and reduce.","title":"Mahout"},{"location":"big_data/#lucenesolr","text":"Lucene, written in Java integrates easily with Hadoop and is a natural companion for Hadoop. It is a tool meant for indexing large blocks of unstructured text. Lucene handles the indexing, while Hadoop handles the distributed queries across the cluster. Lucene-Hadoop features are rapidly evolving as new projects are being developed.","title":"Lucene/Solr"},{"location":"big_data/#avro","text":"Avro is a serialization system that bundles the data together with a schema for understanding it. Each packet comes with a JSON data structure. JSON explains how the data can be parsed. The header of JSON specifies the structure for the data, where the need to write extra tags in the data to mark the fields can be avoided. The output is considerably more compact than the traditional formats like XML.","title":"Avro"},{"location":"big_data/#oozie","text":"A job can be simplified by breaking it into steps. On breaking the project in to multiple Hadoop jobs, Oozie starts processing them in the right sequence. It manages the workflow as specified by DAG (Directed Acyclic Graph) and there is no need for timely monitor.","title":"Oozie"},{"location":"big_data/#sqoop","text":"Apache Sqoop is specially designed to transfer bulk data efficiently from the traditional databases into Hive or HBase. It can also be used to extract data from Hadoop and export it to external structured data-stores like relational databases and enterprise data warehouses. Sqoop is a command line tool, mapping between the tables and the data storage layer, translating the tables into a configurable combination of HDFS, HBase or Hive.","title":"Sqoop"},{"location":"big_data/#flume","text":"Gathering all the data is equal to storing and analyzing it. Apache Flume dispatches \u2018special agents\u2019 to gather information that will be stored in HDFS. The information gathered can be log files, Twitter API, or website scraps. These data can be chained and subjected to analyses.","title":"Flume"},{"location":"big_data/#kafka","text":"Apache Kafka is an open-source stream-processing software. It aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Its storage layer is essentially a \"massively scalable pub/sub message queue designed as a distributed transaction log\".","title":"Kafka"},{"location":"big_data/#zookeeper","text":"Zookeeper is a centralized service that maintains, configures information, gives a name and provides distributed synchronization across a cluster. It imposes a file system-like hierarchy on the cluster and stores all of the metadata for the machines, so we can synchronize the work of the various machines.","title":"Zookeeper"},{"location":"data_model/","text":"Data Model An abstraction that organizes elements of data and how they relate to each other. It can easily translate to database modeling, as this is the essential end state. It's also an iterative process. Data engineers continually reorganize, restructure, and optimize data models to fit the needs of the organization. Data Modeling process: Conceptual Logical Physical SQL Structured Query Language is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS). DDL DDL is short name of Data Definition Language, which deals with database schemas and descriptions, of how the data should reside in the database. CREATE : to create a database and its objects like (table, index, views, store procedure, function, and triggers) ALTER : alters the structure of the existing database DROP : delete objects from the database TRUNCATE : remove all records from a table, including all spaces allocated for the records are removed COMMENT : add comments to the data dictionary RENAME : rename an object DML DML is short name of Data Manipulation Language which deals with data manipulation and includes most common SQL statements such SELECT, INSERT, UPDATE, DELETE, etc., and it is used to store, modify, retrieve, delete and update data in a database. SELECT : retrieve data from a database INSERT : insert data into a table UPDATE : updates existing data within a table DELETE : Delete all records from a database table MERGE : UPSERT operation (insert or update) CALL : call a PL/SQL or Java subprogram EXPLAIN PLAN : interpretation of the data access path LOCK TABLE : concurrency Control DCL DCL is short name of Data Control Language which includes commands such as GRANT and mostly concerned with rights, permissions and other controls of the database system. GRANT : allow users access privileges to the database REVOKE : withdraw users access privileges given by using the GRANT command TCL TCL is short name of Transaction Control Language which deals with a transaction within a database. COMMIT : commits a Transaction ROLLBACK : rollback a transaction in case of any error occurs SAVEPOINT : to rollback the transaction making points within groups SET TRANSACTION : specify characteristics of the transaction Advantages of Using a Relational Database Flexibility for writing in SQL queries: With SQL being the most common database query language. Modeling the data not modeling queries Ability to do JOINS Ability to do aggregations and analytics Secondary Indexes available : You have the advantage of being able to add another index to help with quick searching. Smaller data volumes: If you have a smaller data volume (and not big data) you can use a relational database for its simplicity. ACID Transactions: Allows you to meet a set of properties of database transactions intended to guarantee validity even in the event of errors, power failures, and thus maintain data integrity. Easier to change to business requirements ACID Transactions Properties of database transactions intended to guarantee validity even in the event of errors or power failures. Atomicity : The whole transaction is processed or nothing is processed. A commonly cited example of an atomic transaction is money transactions between two bank accounts. The transaction of transferring money from one account to the other is made up of two operations. First, you have to withdraw money in one account, and second you have to save the withdrawn money to the second account. An atomic transaction, i.e., when either all operations occur or nothing occurs, keeps the database in a consistent state. This ensures that if either of those two operations (withdrawing money from the 1st account or saving the money to the 2nd account) fail, the money is neither lost nor created. Consistency : Only transactions that abide by constraints and rules are written into the database, otherwise the database keeps the previous state. The data should be correct across all rows and tables. Isolation : Transactions are processed independently and securely, order does not matter. A low level of isolation enables many users to access the data simultaneously, however this also increases the possibilities of concurrency effects (e.g., dirty reads or lost updates). On the other hand, a high level of isolation reduces these chances of concurrency effects, but also uses more system resources and transactions blocking each other. Durability : Completed transactions are saved to database even in cases of system failure. A commonly cited example includes tracking flight seat bookings. So once the flight booking records a confirmed seat booking, the seat remains booked even if a system failure occurs When Not to Use a Relational Database Have large amounts of data : Relational Databases are not distributed databases and because of this they can only scale vertically by adding more storage in the machine itself. You are limited by how much you can scale and how much data you can store on one machine. You cannot add more machines like you can in NoSQL databases. Need to be able to store different data type formats : Relational databases are not designed to handle unstructured data. Need high throughput -- fast reads: While ACID transactions bring benefits, they also slow down the process of reading and writing data. If you need very fast reads and writes, using a relational database may not suit your needs. Need a flexible schema : Flexible schema can allow for columns to be added that do not have to be used by every row, saving disk space. Need high availability : The fact that relational databases are not distributed (and even when they are, they have a coordinator/worker architecture), they have a single point of failure. When that database goes down, a fail-over to a backup system occurs and takes time. Need horizontal scalability : Horizontal scalability is the ability to add more machines or nodes to a system to increase performance and space for data. NoSQL NoSQL databases were created to do some of the issues faced with relational databases. Apache Cassandra (partition row store): The data is distributed by partitions across nodes or servers and the data is organized in the columns and rows format. MongoDB (document store): One of the defining characteristics of a document oriented database is that in addition to the key lookups performed by key-value store, the database also offers an API or query language that retrieves document based on its contents. It's basically easy to do search on documents. DynamoDB (key-value store): data is represented as a collection of key and value pairs. Apache HBase (wide column store): It also uses tables, rows and columns but unlike a relational database, the names and formats of the columns can vary from row to row in the same table. Neo4j (graph databas): it's all about relationships and the data is represented as nodes and edges. Apache Cassandra It provides scalability and high availability without compromising performace. Linear Scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data. What type of companies use Apache Cassandra? Uber uses Apache Cassandra for their entire backend. Netflix uses Apache Cassandra to serve all their videos to customers. Good use cases for NoSQL (and more specifically Apache Cassandra) are : Transaction logging (retail, health care) Internet of Things (IoT) Time series data Any workload that is heavy on writes to the database (since Apache Cassandra is optimized for writes). When to use a NoSQL Database Need to be able to store different data type formats : NoSQL was also created to handle different data configurations: structured, semi-structured, and unstructured data. JSON, XML documents can all be handled easily with NoSQL. Large amounts of data : Relational Databases are not distributed databases and because of this they can only scale vertically by adding more storage in the machine itself. NoSQL databases were created to be able to be horizontally scalable. The more servers/systems you add to the database the more data that can be hosted with high availability and low latency (fast reads and writes). Need horizontal scalability : Horizontal scalability is the ability to add more machines or nodes to a system to increase performance and space for data Need high throughput : While ACID transactions bring benefits they also slow down the process of reading and writing data. If you need very fast reads and writes using a relational database may not suit your needs. Need a flexible schema : Flexible schema can allow for columns to be added that do not have to be used by every row, saving disk space. Need high availability : Relational databases have a single point of failure. When that database goes down, a failover to a backup system must happen and takes time. When NOT to use a NoSQL Database? When you have a small dataset : NoSQL databases were made for big datasets not small datasets and while it works it wasn\u2019t created for that. When you need ACID Transactions : If you need a consistent database with ACID transactions, then most NoSQL databases will not be able to serve this need. NoSQL database are eventually consistent and do not provide ACID transactions. However, there are exceptions to it. Some non-relational databases like MongoDB can support ACID transactions. When you need the ability to do JOINS across tables : NoSQL does not allow the ability to do JOINS. This is not allowed as this will result in full table scans. If you want to be able to do aggregations and analytics If you have changing business requirements: Ad-hoc queries are possible but difficult as the data model was done to fix particular queries If your queries are not available and you need the flexibility : You need your queries in advance. If those are not available or you will need to be able to have flexibility on how you query your data you might need to stick with a relational database. Relational Data Model Importance of Relational Databases: Standardization of data model : Once your data is transformed into the rows and columns format, your data is standardized and you can query it with SQL Flexibility in adding and altering tables : Relational databases gives you flexibility to add tables, alter tables, add and remove data. Data Integrity : Data Integrity is the backbone of using a relational database. Standard Query Language (SQL) : A standard language can be used to access the data with a predefined language. Simplicity : Data is systematically stored and modeled in tabular format. Intuitive Organization : The spreadsheet format is intuitive but intuitive to data modeling in relational databases. Online Analytical Processing (OLAP) Databases optimized for these workloads allow for complex analytical and ad hoc queries, including aggregations. These type of databases are optimized for reads. Online Transactional Processing (OLTP) Databases optimized for these workloads allow for less complex queries in large volume. The types of queries for these databases are read, insert, update, and delete. The key to remember the difference between OLAP and OLTP is analytics (A) vs transactions (T). If you want to get the price of a shoe then you are using OLTP (this has very little or no aggregations). If you want to know the total stock of shoes a particular store sold, then this requires using OLAP (since this will require aggregations). Objectives of Normal Form To free the database from unwanted insertions, updates, & deletion dependencies To reduce the need for refactoring the database as new types of data are introduced To make the relational model more informative to users To make the database neutral to the query statistics How to reach First Normal Form (1NF): Atomic values: each cell contains unique and single values Be able to add data without altering tables Separate different relations into different tables Keep relationships between tables together with foreign keys Second Normal Form (2NF): Have reached 1NF All columns in the table must rely on the Primary Key Third Normal Form (3NF): Must be in 2nd Normal Form No transitive dependencies Remember, transitive dependencies you are trying to maintain is that to get from A-> C, you want to avoid going through B. Denormalization JOINS on the database allow for outstanding flexibility but are extremely slow. If you are dealing with heavy reads on your database, you may want to think about denormalizing your tables. You get your data into normalized form, and then you proceed with denormalization. So, denormalization comes after normalization. Fact and Dimension Table As you can see in the image, the unique primary key for each Dimension table is included in the Fact table. In this example, it helps to think about the Dimension tables providing the following information: Where the product was bought? (Dim_Store table) When the product was bought? (Dim_Date table) What product was bought? (Dim_Product table) The Fact table provides the metric of the business process (here Sales). How many units of products were bought? (Fact_Sales table) CAP Theorem A theorem in computer science that states it is impossible for a distributed data store to simulataneously provide more than two out of the following three guarantees of consisency, availability and partition tolerance. Consistency : Every read from the database gets the latest (and correct) piece of data or an error Availability : Every request is received and a response is given -- without a guarantee that the data is the latest update Partition Tolerance : The system continues to work regardless of losing network connectivity between nodes Data Modeling in Apache Cassandra: Denormalization is not just okay -- it's a must Denormalization must be done for fast reads Apache Cassandra has been optimized for fast writes ALWAYS think Queries first One table per query is a great strategy Apache Cassandra does not allow for JOINs between tables","title":"Data Model"},{"location":"data_model/#data-model","text":"An abstraction that organizes elements of data and how they relate to each other. It can easily translate to database modeling, as this is the essential end state. It's also an iterative process. Data engineers continually reorganize, restructure, and optimize data models to fit the needs of the organization. Data Modeling process: Conceptual Logical Physical","title":"Data Model"},{"location":"data_model/#sql","text":"Structured Query Language is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS).","title":"SQL"},{"location":"data_model/#ddl","text":"DDL is short name of Data Definition Language, which deals with database schemas and descriptions, of how the data should reside in the database. CREATE : to create a database and its objects like (table, index, views, store procedure, function, and triggers) ALTER : alters the structure of the existing database DROP : delete objects from the database TRUNCATE : remove all records from a table, including all spaces allocated for the records are removed COMMENT : add comments to the data dictionary RENAME : rename an object","title":"DDL"},{"location":"data_model/#dml","text":"DML is short name of Data Manipulation Language which deals with data manipulation and includes most common SQL statements such SELECT, INSERT, UPDATE, DELETE, etc., and it is used to store, modify, retrieve, delete and update data in a database. SELECT : retrieve data from a database INSERT : insert data into a table UPDATE : updates existing data within a table DELETE : Delete all records from a database table MERGE : UPSERT operation (insert or update) CALL : call a PL/SQL or Java subprogram EXPLAIN PLAN : interpretation of the data access path LOCK TABLE : concurrency Control","title":"DML"},{"location":"data_model/#dcl","text":"DCL is short name of Data Control Language which includes commands such as GRANT and mostly concerned with rights, permissions and other controls of the database system. GRANT : allow users access privileges to the database REVOKE : withdraw users access privileges given by using the GRANT command","title":"DCL"},{"location":"data_model/#tcl","text":"TCL is short name of Transaction Control Language which deals with a transaction within a database. COMMIT : commits a Transaction ROLLBACK : rollback a transaction in case of any error occurs SAVEPOINT : to rollback the transaction making points within groups SET TRANSACTION : specify characteristics of the transaction","title":"TCL"},{"location":"data_model/#advantages-of-using-a-relational-database","text":"Flexibility for writing in SQL queries: With SQL being the most common database query language. Modeling the data not modeling queries Ability to do JOINS Ability to do aggregations and analytics Secondary Indexes available : You have the advantage of being able to add another index to help with quick searching. Smaller data volumes: If you have a smaller data volume (and not big data) you can use a relational database for its simplicity. ACID Transactions: Allows you to meet a set of properties of database transactions intended to guarantee validity even in the event of errors, power failures, and thus maintain data integrity. Easier to change to business requirements","title":"Advantages of Using a Relational Database"},{"location":"data_model/#acid-transactions","text":"Properties of database transactions intended to guarantee validity even in the event of errors or power failures. Atomicity : The whole transaction is processed or nothing is processed. A commonly cited example of an atomic transaction is money transactions between two bank accounts. The transaction of transferring money from one account to the other is made up of two operations. First, you have to withdraw money in one account, and second you have to save the withdrawn money to the second account. An atomic transaction, i.e., when either all operations occur or nothing occurs, keeps the database in a consistent state. This ensures that if either of those two operations (withdrawing money from the 1st account or saving the money to the 2nd account) fail, the money is neither lost nor created. Consistency : Only transactions that abide by constraints and rules are written into the database, otherwise the database keeps the previous state. The data should be correct across all rows and tables. Isolation : Transactions are processed independently and securely, order does not matter. A low level of isolation enables many users to access the data simultaneously, however this also increases the possibilities of concurrency effects (e.g., dirty reads or lost updates). On the other hand, a high level of isolation reduces these chances of concurrency effects, but also uses more system resources and transactions blocking each other. Durability : Completed transactions are saved to database even in cases of system failure. A commonly cited example includes tracking flight seat bookings. So once the flight booking records a confirmed seat booking, the seat remains booked even if a system failure occurs","title":"ACID Transactions"},{"location":"data_model/#when-not-to-use-a-relational-database","text":"Have large amounts of data : Relational Databases are not distributed databases and because of this they can only scale vertically by adding more storage in the machine itself. You are limited by how much you can scale and how much data you can store on one machine. You cannot add more machines like you can in NoSQL databases. Need to be able to store different data type formats : Relational databases are not designed to handle unstructured data. Need high throughput -- fast reads: While ACID transactions bring benefits, they also slow down the process of reading and writing data. If you need very fast reads and writes, using a relational database may not suit your needs. Need a flexible schema : Flexible schema can allow for columns to be added that do not have to be used by every row, saving disk space. Need high availability : The fact that relational databases are not distributed (and even when they are, they have a coordinator/worker architecture), they have a single point of failure. When that database goes down, a fail-over to a backup system occurs and takes time. Need horizontal scalability : Horizontal scalability is the ability to add more machines or nodes to a system to increase performance and space for data.","title":"When Not to Use a Relational Database"},{"location":"data_model/#nosql","text":"NoSQL databases were created to do some of the issues faced with relational databases. Apache Cassandra (partition row store): The data is distributed by partitions across nodes or servers and the data is organized in the columns and rows format. MongoDB (document store): One of the defining characteristics of a document oriented database is that in addition to the key lookups performed by key-value store, the database also offers an API or query language that retrieves document based on its contents. It's basically easy to do search on documents. DynamoDB (key-value store): data is represented as a collection of key and value pairs. Apache HBase (wide column store): It also uses tables, rows and columns but unlike a relational database, the names and formats of the columns can vary from row to row in the same table. Neo4j (graph databas): it's all about relationships and the data is represented as nodes and edges.","title":"NoSQL"},{"location":"data_model/#apache-cassandra","text":"It provides scalability and high availability without compromising performace. Linear Scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data. What type of companies use Apache Cassandra? Uber uses Apache Cassandra for their entire backend. Netflix uses Apache Cassandra to serve all their videos to customers. Good use cases for NoSQL (and more specifically Apache Cassandra) are : Transaction logging (retail, health care) Internet of Things (IoT) Time series data Any workload that is heavy on writes to the database (since Apache Cassandra is optimized for writes).","title":"Apache Cassandra"},{"location":"data_model/#when-to-use-a-nosql-database","text":"Need to be able to store different data type formats : NoSQL was also created to handle different data configurations: structured, semi-structured, and unstructured data. JSON, XML documents can all be handled easily with NoSQL. Large amounts of data : Relational Databases are not distributed databases and because of this they can only scale vertically by adding more storage in the machine itself. NoSQL databases were created to be able to be horizontally scalable. The more servers/systems you add to the database the more data that can be hosted with high availability and low latency (fast reads and writes). Need horizontal scalability : Horizontal scalability is the ability to add more machines or nodes to a system to increase performance and space for data Need high throughput : While ACID transactions bring benefits they also slow down the process of reading and writing data. If you need very fast reads and writes using a relational database may not suit your needs. Need a flexible schema : Flexible schema can allow for columns to be added that do not have to be used by every row, saving disk space. Need high availability : Relational databases have a single point of failure. When that database goes down, a failover to a backup system must happen and takes time.","title":"When to use a NoSQL Database"},{"location":"data_model/#when-not-to-use-a-nosql-database","text":"When you have a small dataset : NoSQL databases were made for big datasets not small datasets and while it works it wasn\u2019t created for that. When you need ACID Transactions : If you need a consistent database with ACID transactions, then most NoSQL databases will not be able to serve this need. NoSQL database are eventually consistent and do not provide ACID transactions. However, there are exceptions to it. Some non-relational databases like MongoDB can support ACID transactions. When you need the ability to do JOINS across tables : NoSQL does not allow the ability to do JOINS. This is not allowed as this will result in full table scans. If you want to be able to do aggregations and analytics If you have changing business requirements: Ad-hoc queries are possible but difficult as the data model was done to fix particular queries If your queries are not available and you need the flexibility : You need your queries in advance. If those are not available or you will need to be able to have flexibility on how you query your data you might need to stick with a relational database.","title":"When NOT to use a NoSQL Database?"},{"location":"data_model/#relational-data-model","text":"Importance of Relational Databases: Standardization of data model : Once your data is transformed into the rows and columns format, your data is standardized and you can query it with SQL Flexibility in adding and altering tables : Relational databases gives you flexibility to add tables, alter tables, add and remove data. Data Integrity : Data Integrity is the backbone of using a relational database. Standard Query Language (SQL) : A standard language can be used to access the data with a predefined language. Simplicity : Data is systematically stored and modeled in tabular format. Intuitive Organization : The spreadsheet format is intuitive but intuitive to data modeling in relational databases.","title":"Relational Data Model"},{"location":"data_model/#online-analytical-processing-olap","text":"Databases optimized for these workloads allow for complex analytical and ad hoc queries, including aggregations. These type of databases are optimized for reads.","title":"Online Analytical Processing (OLAP)"},{"location":"data_model/#online-transactional-processing-oltp","text":"Databases optimized for these workloads allow for less complex queries in large volume. The types of queries for these databases are read, insert, update, and delete. The key to remember the difference between OLAP and OLTP is analytics (A) vs transactions (T). If you want to get the price of a shoe then you are using OLTP (this has very little or no aggregations). If you want to know the total stock of shoes a particular store sold, then this requires using OLAP (since this will require aggregations).","title":"Online Transactional Processing (OLTP)"},{"location":"data_model/#objectives-of-normal-form","text":"To free the database from unwanted insertions, updates, & deletion dependencies To reduce the need for refactoring the database as new types of data are introduced To make the relational model more informative to users To make the database neutral to the query statistics How to reach First Normal Form (1NF): Atomic values: each cell contains unique and single values Be able to add data without altering tables Separate different relations into different tables Keep relationships between tables together with foreign keys Second Normal Form (2NF): Have reached 1NF All columns in the table must rely on the Primary Key Third Normal Form (3NF): Must be in 2nd Normal Form No transitive dependencies Remember, transitive dependencies you are trying to maintain is that to get from A-> C, you want to avoid going through B.","title":"Objectives of Normal Form"},{"location":"data_model/#denormalization","text":"JOINS on the database allow for outstanding flexibility but are extremely slow. If you are dealing with heavy reads on your database, you may want to think about denormalizing your tables. You get your data into normalized form, and then you proceed with denormalization. So, denormalization comes after normalization.","title":"Denormalization"},{"location":"data_model/#fact-and-dimension-table","text":"As you can see in the image, the unique primary key for each Dimension table is included in the Fact table. In this example, it helps to think about the Dimension tables providing the following information: Where the product was bought? (Dim_Store table) When the product was bought? (Dim_Date table) What product was bought? (Dim_Product table) The Fact table provides the metric of the business process (here Sales). How many units of products were bought? (Fact_Sales table)","title":"Fact and Dimension Table"},{"location":"data_model/#cap-theorem","text":"A theorem in computer science that states it is impossible for a distributed data store to simulataneously provide more than two out of the following three guarantees of consisency, availability and partition tolerance. Consistency : Every read from the database gets the latest (and correct) piece of data or an error Availability : Every request is received and a response is given -- without a guarantee that the data is the latest update Partition Tolerance : The system continues to work regardless of losing network connectivity between nodes","title":"CAP Theorem"},{"location":"data_model/#data-modeling-in-apache-cassandra","text":"Denormalization is not just okay -- it's a must Denormalization must be done for fast reads Apache Cassandra has been optimized for fast writes ALWAYS think Queries first One table per query is a great strategy Apache Cassandra does not allow for JOINs between tables","title":"Data Modeling in Apache Cassandra:"},{"location":"data_science/","text":"Questions Data Science Answers Is it A or B? : Classfication algorithms whether a coupon or discount bring more customer. Is this weird? : Anomaly detection algorithms credit card spending habit, flags unsual behaviour. How much? How many? : Regression algorithms what will my nth quarter sales be. How is this organized? : Clustering algorithms which viewers like same type of movies, which printer model fails same way. What should I do now? : Reinforcement Learning algorithms self driving car, whether to brake or accelerate at yellow light. Is data ready for Data Science Relevant? Connected? Accurate? Enough to work with? New model building workflow Gather data Feature extraction Pick ML framework Spin up AWS EC2 instance Setup machine Launch training job Analyze results When you have a standard computer, and you want to run multiple processes on it, the operating system is what deals with the scheduling - it determines which process gets access to which processor at a given point in time, which process gets access to which parts of memory, decides which processes should be frozen or die in cases where there is a shortage of resources. Use Docker to containerize model development environment, setup code and dependencies on each remote machine. Kubernetes is an open-source container-orchestration system for automating application deployment, scaling and management. These tools allow for easy to use debugging and monitoring. Borg is a Cluster OS Google uses for managing internal workloads. It manages thousands of machines with very good internal network connectivity. When you want to run a \u201cservice\u201d - which is the equivalent of a \u201cdaemon\u201d on a normal machine, something that should stay up all the time - you \u201crun it on Borg\u201d - that is, you tell the Borg cluster scheduler that you want, say, 200 copies of the binary that determines the service. The Borg scheduler identifies machines in the cluster that will have the spare capacity to run your service and sends a request to run the service to the node agents on these machines. Descriptive Statistics Descriptive statistics summarize a given data set, which can be either a representation of the entire or a sample of a population. It is broken down into measure of central tendency (mean, median & mode) and measure of variability (standard deviation, variance & skewness). Inferential Statistics It is the process of using data analysis to deduce properties of an underlying probability distribution. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and driving estimates. It is assumed that the observed data set is sampled from a larger population.","title":"Data Science"},{"location":"data_science/#questions-data-science-answers","text":"Is it A or B? : Classfication algorithms whether a coupon or discount bring more customer. Is this weird? : Anomaly detection algorithms credit card spending habit, flags unsual behaviour. How much? How many? : Regression algorithms what will my nth quarter sales be. How is this organized? : Clustering algorithms which viewers like same type of movies, which printer model fails same way. What should I do now? : Reinforcement Learning algorithms self driving car, whether to brake or accelerate at yellow light.","title":"Questions Data Science Answers"},{"location":"data_science/#is-data-ready-for-data-science","text":"Relevant? Connected? Accurate? Enough to work with?","title":"Is data ready for Data Science"},{"location":"data_science/#new-model-building-workflow","text":"Gather data Feature extraction Pick ML framework Spin up AWS EC2 instance Setup machine Launch training job Analyze results When you have a standard computer, and you want to run multiple processes on it, the operating system is what deals with the scheduling - it determines which process gets access to which processor at a given point in time, which process gets access to which parts of memory, decides which processes should be frozen or die in cases where there is a shortage of resources. Use Docker to containerize model development environment, setup code and dependencies on each remote machine. Kubernetes is an open-source container-orchestration system for automating application deployment, scaling and management. These tools allow for easy to use debugging and monitoring. Borg is a Cluster OS Google uses for managing internal workloads. It manages thousands of machines with very good internal network connectivity. When you want to run a \u201cservice\u201d - which is the equivalent of a \u201cdaemon\u201d on a normal machine, something that should stay up all the time - you \u201crun it on Borg\u201d - that is, you tell the Borg cluster scheduler that you want, say, 200 copies of the binary that determines the service. The Borg scheduler identifies machines in the cluster that will have the spare capacity to run your service and sends a request to run the service to the node agents on these machines.","title":"New model building workflow"},{"location":"data_science/#descriptive-statistics","text":"Descriptive statistics summarize a given data set, which can be either a representation of the entire or a sample of a population. It is broken down into measure of central tendency (mean, median & mode) and measure of variability (standard deviation, variance & skewness).","title":"Descriptive Statistics"},{"location":"data_science/#inferential-statistics","text":"It is the process of using data analysis to deduce properties of an underlying probability distribution. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and driving estimates. It is assumed that the observed data set is sampled from a larger population.","title":"Inferential Statistics"},{"location":"data_warehouse/","text":"What Is A Data Warehouse? It is a system (including processes, technologies & data representations) that enables us to support analytical processes. A Business Perspective You are in charge of a retailer\u2019s data infrastructure. Let\u2019s look at some business activities. Customers should be able to find goods & make orders Inventory Staff should be able to stock, retrieve, and re-order goods Delivery Staff should be able to pick up & deliver goods HR should be able to assess the performance of sales staff Marketing should be able to see the effect of different sales channels Management should be able to monitor sales growth Ask yourself: Can I build a database to support these activities? Are all of the above questions of the same nature? Let's take a closer look at details that may affect your data infrastructure. Retailer has a nation-wide presence \u2192 Scale? Acquired smaller retailers, brick & mortar shops, online store \u2192 Single database? Complexity? Has support call center & social media accounts \u2192 Tabular data? Customers, Inventory Staff and Delivery staff expect the system to be fast & stable \u2192 Performance HR, Marketing & Sales Reports want a lot information but have not decided yet on everything they need \u2192 Clear Requirements? A Technical Perspective A data warehouse is a copy of transaction data specifically structured for query and analysis. A data warehouse is a subject-oriented, integrated, nonvolatile and time-variant collection of data in support of management's decisions. A data warehouse is a system that retrieves and consolidates data periodically from source systems into a dimensional or normalized data store. It usually keeps years of history and is queried for business intelligence or other analytical activities. It is typically updated in batches, not every time a transaction happens in the source system. Data Warehouse Goals Simple to understand Performant Quality Assured Handles new questions well Secure Facts & Dimenstions If you are unsure if a column is a fact or dimension, the simplest rule is that a fact is usually: Numeric & Additive Fact tables: Record business events, like an order, a phone call, a book review Fact tables columns record events recorded in quantifiable metrics like quantity of an item, duration of a call, a book rating. Dimension tables: Record the context of the business events, e.g: who, what, where, why etc. Dimension tables columns contain attributes like the store at which an item is purchased, or the customer who make the call, etc.","title":"Data Warehouse"},{"location":"data_warehouse/#what-is-a-data-warehouse","text":"It is a system (including processes, technologies & data representations) that enables us to support analytical processes.","title":"What Is A Data Warehouse?"},{"location":"data_warehouse/#a-business-perspective","text":"You are in charge of a retailer\u2019s data infrastructure. Let\u2019s look at some business activities. Customers should be able to find goods & make orders Inventory Staff should be able to stock, retrieve, and re-order goods Delivery Staff should be able to pick up & deliver goods HR should be able to assess the performance of sales staff Marketing should be able to see the effect of different sales channels Management should be able to monitor sales growth Ask yourself: Can I build a database to support these activities? Are all of the above questions of the same nature? Let's take a closer look at details that may affect your data infrastructure. Retailer has a nation-wide presence \u2192 Scale? Acquired smaller retailers, brick & mortar shops, online store \u2192 Single database? Complexity? Has support call center & social media accounts \u2192 Tabular data? Customers, Inventory Staff and Delivery staff expect the system to be fast & stable \u2192 Performance HR, Marketing & Sales Reports want a lot information but have not decided yet on everything they need \u2192 Clear Requirements?","title":"A Business Perspective"},{"location":"data_warehouse/#a-technical-perspective","text":"A data warehouse is a copy of transaction data specifically structured for query and analysis. A data warehouse is a subject-oriented, integrated, nonvolatile and time-variant collection of data in support of management's decisions. A data warehouse is a system that retrieves and consolidates data periodically from source systems into a dimensional or normalized data store. It usually keeps years of history and is queried for business intelligence or other analytical activities. It is typically updated in batches, not every time a transaction happens in the source system.","title":"A Technical Perspective"},{"location":"data_warehouse/#data-warehouse-goals","text":"Simple to understand Performant Quality Assured Handles new questions well Secure","title":"Data Warehouse Goals"},{"location":"data_warehouse/#facts-dimenstions","text":"If you are unsure if a column is a fact or dimension, the simplest rule is that a fact is usually: Numeric & Additive Fact tables: Record business events, like an order, a phone call, a book review Fact tables columns record events recorded in quantifiable metrics like quantity of an item, duration of a call, a book rating. Dimension tables: Record the context of the business events, e.g: who, what, where, why etc. Dimension tables columns contain attributes like the store at which an item is purchased, or the customer who make the call, etc.","title":"Facts &amp; Dimenstions"},{"location":"libraries/","text":"","title":"Libraries"},{"location":"terminology/","text":"","title":"Terminology"}]}