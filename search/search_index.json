{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"About"},{"location":"big_data/","text":"Big Data Hadoop Hadoop is an open source software platform for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware. YARN YARN is a resource manager that schedules jobs across a cluster. The manager keeps track of what computer resources are available and then assigns those resources to specific tasks. Ambari Ambari is an Apache project supported by Hortonworks. It offers a web based GUI (Graphical User Interface) with wizard scripts for setting up clusters with most of the standard components. Ambari provisions, manages and monitors all the clusters of Hadoop jobs. MapReduce MapReduce distributes the processing of data on your cluster. Divides your data up intro partitions that are MAPPED (transformed) and REDUCED (aggregated) by mapper and reducer functions you define. MapReduce is a programming model and an associated implementation for processing and generating big data sets with a parallel, distributed algorithm on a cluster. A MapReduce program is composed of a map procedure (or method) - which performs filtering and sorting (such as sorting students by first name into queues, one queue for each name), and a reduce method - which performs a summary operation (such as counting the number of students in each queue, yielding name frequencies). The \"MapReduce System\" (also called \"infrastructure\" or \"framework\") orchestrates the processing by marshalling the distributed servers, running the various tasks in parallel, managing all communications and data transfers between the various parts of the system, and providing for redundancy and fault tolerance. In computer science, marshalling is the process of transforming the memory representation of an object to a data format suitable for storage or transmission, and it is typically used when data must be moved between different parts of a computer program or from one program to another. Marshalling is similar to serialization and is used to communicate to remote objects with an object, in this case a serialized object. MapReduce is a programming technique for manipulating large data sets. \"Hadoop MapReduce\" is a specific implementation of this programming technique. The technique works by first dividing up a large dataset and distributing the data across a cluster. In the map step, each data is analyzed and converted into a (key, value) pair. Then these key-value pairs are shuffled across the cluster so that all keys are on the same machine. In the reduce step, the values with the same keys are combined together. HDFS (Hadoop Distributed File System) The HDFS, distributed under Apache license offers a basic framework for splitting up data collections between multiple nodes. In HDFS, the large files are broken into blocks, where several nodes hold all of the blocks from a file. The file system is designed in a way to mix fault tolerance with high throughput. The blocks of HDFS are loaded to maintain steady streaming. They are not usually cached to minimize latency. In other words it is a big data storage system that splits data into chunks and stores the chunks across a cluster of computers. Pig When the data stored is visible to Hadoop, Apache Pig dives into the data and runs the code that is written in its own language, called Pig Latin. Pig Latin is filled with abstractions for handling the data. Pig comes with standard functions for common tasks like averaging data, working with dates, or to find differences between strings. Pig also allows the user to write languages of their own, called UDF (User Defined Function), when the standard functions fall short. Hive If you are already fluent with SQL, then you can leverage Hadoop using Hive. Hive was developed by some folks at Facebook. Apache Hive regulates the process of extracting bits from all the files in HBase. It supports analysis of large datasets stored in Hadoop\u2019s HDFS and compatible file systems. It also provides an SQL like language called HSQL (HiveSQL) that gets into the files and extracts the required snippets for the code. TEZ A Framework for YARN-based, Data Processing Applications in Hadoop. Apache Tez is an extensible framework for building high performance batch and interactive data processing applications, coordinated by YARN in Apache Hadoop. Tez improves the MapReduce paradigm by dramatically improving its speed, while maintaining MapReduce\u2019s ability to scale to petabytes of data. Important Hadoop ecosystem projects like Apache Hive and Apache Pig use Apache Tez. Spark Spark is the next generation that pretty much works like Hadoop that processes data cached in the memory. Its objective is to make data analysis fast to run and write with a general execution model. This can optimize arbitrary operator graphs and support in-memory computing, which lets it query data faster than disk-based engines like Hadoop. It also supports streaming data, run ML algorithms on clusters. HBase HBase is a column-oriented database management system that runs on top of HDFS. HBase applications are written in Java, very much like the MapReduce application. It comprises a set of tables, where each table contains rows and columns like a traditional database. When the data falls into the big table, HBase will store the data, search it and automatically share the table across multiple nodes so that MapReduce jobs can run it locally. HBase offers a limited guarantee for some local changes. The changes that happen in a single row can succeed or fail at the same time. It has a dynamic data model and not a relational DBMS. Storm A system for processing streaming data in real time. Apache Storm adds reliable real-time data processing capabilities to Enterprise Hadoop. Storm on YARN is powerful for scenarios requiring real-time analytics, machine learning and continuous monitoring of operations. NoSQL Some Hadoop clusters integrate with NoSQL data stores that come with their own mechanisms for storing data across a cluster of nodes. This allows them to store and retrieve data with all the features of the NoSQL database, after which Hadoop can be used to schedule data analysis jobs on the same cluster. Mahout Mahout is designed to implement a great number of algorithms, classifications and filtering of data analysis to Hadoop cluster. Many of the standard algorithms like K-means, Dirichelet, parallel pattern and Bayesian classifications are ready to run on the data with a Hadoop style Map and reduce. Lucene/Solr Lucene, written in Java integrates easily with Hadoop and is a natural companion for Hadoop. It is a tool meant for indexing large blocks of unstructured text. Lucene handles the indexing, while Hadoop handles the distributed queries across the cluster. Lucene-Hadoop features are rapidly evolving as new projects are being developed. Avro Avro is a serialization system that bundles the data together with a schema for understanding it. Each packet comes with a JSON data structure. JSON explains how the data can be parsed. The header of JSON specifies the structure for the data, where the need to write extra tags in the data to mark the fields can be avoided. The output is considerably more compact than the traditional formats like XML. Oozie A job can be simplified by breaking it into steps. On breaking the project in to multiple Hadoop jobs, Oozie starts processing them in the right sequence. It manages the workflow as specified by DAG (Directed Acyclic Graph) and there is no need for timely monitor. Sqoop Apache Sqoop is specially designed to transfer bulk data efficiently from the traditional databases into Hive or HBase. It can also be used to extract data from Hadoop and export it to external structured data-stores like relational databases and enterprise data warehouses. Sqoop is a command line tool, mapping between the tables and the data storage layer, translating the tables into a configurable combination of HDFS, HBase or Hive. Flume Gathering all the data is equal to storing and analyzing it. Apache Flume dispatches \u2018special agents\u2019 to gather information that will be stored in HDFS. The information gathered can be log files, Twitter API, or website scraps. These data can be chained and subjected to analyses. Kafka Apache Kafka is an open-source stream-processing software. It aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Its storage layer is essentially a \"massively scalable pub/sub message queue designed as a distributed transaction log\". Zookeeper Zookeeper is a centralized service that maintains, configures information, gives a name and provides distributed synchronization across a cluster. It imposes a file system-like hierarchy on the cluster and stores all of the metadata for the machines, so we can synchronize the work of the various machines.","title":"Big Data"},{"location":"big_data/#big-data","text":"","title":"Big Data"},{"location":"big_data/#hadoop","text":"Hadoop is an open source software platform for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware.","title":"Hadoop"},{"location":"big_data/#yarn","text":"YARN is a resource manager that schedules jobs across a cluster. The manager keeps track of what computer resources are available and then assigns those resources to specific tasks.","title":"YARN"},{"location":"big_data/#ambari","text":"Ambari is an Apache project supported by Hortonworks. It offers a web based GUI (Graphical User Interface) with wizard scripts for setting up clusters with most of the standard components. Ambari provisions, manages and monitors all the clusters of Hadoop jobs.","title":"Ambari"},{"location":"big_data/#mapreduce","text":"MapReduce distributes the processing of data on your cluster. Divides your data up intro partitions that are MAPPED (transformed) and REDUCED (aggregated) by mapper and reducer functions you define. MapReduce is a programming model and an associated implementation for processing and generating big data sets with a parallel, distributed algorithm on a cluster. A MapReduce program is composed of a map procedure (or method) - which performs filtering and sorting (such as sorting students by first name into queues, one queue for each name), and a reduce method - which performs a summary operation (such as counting the number of students in each queue, yielding name frequencies). The \"MapReduce System\" (also called \"infrastructure\" or \"framework\") orchestrates the processing by marshalling the distributed servers, running the various tasks in parallel, managing all communications and data transfers between the various parts of the system, and providing for redundancy and fault tolerance. In computer science, marshalling is the process of transforming the memory representation of an object to a data format suitable for storage or transmission, and it is typically used when data must be moved between different parts of a computer program or from one program to another. Marshalling is similar to serialization and is used to communicate to remote objects with an object, in this case a serialized object. MapReduce is a programming technique for manipulating large data sets. \"Hadoop MapReduce\" is a specific implementation of this programming technique. The technique works by first dividing up a large dataset and distributing the data across a cluster. In the map step, each data is analyzed and converted into a (key, value) pair. Then these key-value pairs are shuffled across the cluster so that all keys are on the same machine. In the reduce step, the values with the same keys are combined together.","title":"MapReduce"},{"location":"big_data/#hdfs-hadoop-distributed-file-system","text":"The HDFS, distributed under Apache license offers a basic framework for splitting up data collections between multiple nodes. In HDFS, the large files are broken into blocks, where several nodes hold all of the blocks from a file. The file system is designed in a way to mix fault tolerance with high throughput. The blocks of HDFS are loaded to maintain steady streaming. They are not usually cached to minimize latency. In other words it is a big data storage system that splits data into chunks and stores the chunks across a cluster of computers.","title":"HDFS (Hadoop Distributed File System)"},{"location":"big_data/#pig","text":"When the data stored is visible to Hadoop, Apache Pig dives into the data and runs the code that is written in its own language, called Pig Latin. Pig Latin is filled with abstractions for handling the data. Pig comes with standard functions for common tasks like averaging data, working with dates, or to find differences between strings. Pig also allows the user to write languages of their own, called UDF (User Defined Function), when the standard functions fall short.","title":"Pig"},{"location":"big_data/#hive","text":"If you are already fluent with SQL, then you can leverage Hadoop using Hive. Hive was developed by some folks at Facebook. Apache Hive regulates the process of extracting bits from all the files in HBase. It supports analysis of large datasets stored in Hadoop\u2019s HDFS and compatible file systems. It also provides an SQL like language called HSQL (HiveSQL) that gets into the files and extracts the required snippets for the code.","title":"Hive"},{"location":"big_data/#tez","text":"A Framework for YARN-based, Data Processing Applications in Hadoop. Apache Tez is an extensible framework for building high performance batch and interactive data processing applications, coordinated by YARN in Apache Hadoop. Tez improves the MapReduce paradigm by dramatically improving its speed, while maintaining MapReduce\u2019s ability to scale to petabytes of data. Important Hadoop ecosystem projects like Apache Hive and Apache Pig use Apache Tez.","title":"TEZ"},{"location":"big_data/#spark","text":"Spark is the next generation that pretty much works like Hadoop that processes data cached in the memory. Its objective is to make data analysis fast to run and write with a general execution model. This can optimize arbitrary operator graphs and support in-memory computing, which lets it query data faster than disk-based engines like Hadoop. It also supports streaming data, run ML algorithms on clusters.","title":"Spark"},{"location":"big_data/#hbase","text":"HBase is a column-oriented database management system that runs on top of HDFS. HBase applications are written in Java, very much like the MapReduce application. It comprises a set of tables, where each table contains rows and columns like a traditional database. When the data falls into the big table, HBase will store the data, search it and automatically share the table across multiple nodes so that MapReduce jobs can run it locally. HBase offers a limited guarantee for some local changes. The changes that happen in a single row can succeed or fail at the same time. It has a dynamic data model and not a relational DBMS.","title":"HBase"},{"location":"big_data/#storm","text":"A system for processing streaming data in real time. Apache Storm adds reliable real-time data processing capabilities to Enterprise Hadoop. Storm on YARN is powerful for scenarios requiring real-time analytics, machine learning and continuous monitoring of operations.","title":"Storm"},{"location":"big_data/#nosql","text":"Some Hadoop clusters integrate with NoSQL data stores that come with their own mechanisms for storing data across a cluster of nodes. This allows them to store and retrieve data with all the features of the NoSQL database, after which Hadoop can be used to schedule data analysis jobs on the same cluster.","title":"NoSQL"},{"location":"big_data/#mahout","text":"Mahout is designed to implement a great number of algorithms, classifications and filtering of data analysis to Hadoop cluster. Many of the standard algorithms like K-means, Dirichelet, parallel pattern and Bayesian classifications are ready to run on the data with a Hadoop style Map and reduce.","title":"Mahout"},{"location":"big_data/#lucenesolr","text":"Lucene, written in Java integrates easily with Hadoop and is a natural companion for Hadoop. It is a tool meant for indexing large blocks of unstructured text. Lucene handles the indexing, while Hadoop handles the distributed queries across the cluster. Lucene-Hadoop features are rapidly evolving as new projects are being developed.","title":"Lucene/Solr"},{"location":"big_data/#avro","text":"Avro is a serialization system that bundles the data together with a schema for understanding it. Each packet comes with a JSON data structure. JSON explains how the data can be parsed. The header of JSON specifies the structure for the data, where the need to write extra tags in the data to mark the fields can be avoided. The output is considerably more compact than the traditional formats like XML.","title":"Avro"},{"location":"big_data/#oozie","text":"A job can be simplified by breaking it into steps. On breaking the project in to multiple Hadoop jobs, Oozie starts processing them in the right sequence. It manages the workflow as specified by DAG (Directed Acyclic Graph) and there is no need for timely monitor.","title":"Oozie"},{"location":"big_data/#sqoop","text":"Apache Sqoop is specially designed to transfer bulk data efficiently from the traditional databases into Hive or HBase. It can also be used to extract data from Hadoop and export it to external structured data-stores like relational databases and enterprise data warehouses. Sqoop is a command line tool, mapping between the tables and the data storage layer, translating the tables into a configurable combination of HDFS, HBase or Hive.","title":"Sqoop"},{"location":"big_data/#flume","text":"Gathering all the data is equal to storing and analyzing it. Apache Flume dispatches \u2018special agents\u2019 to gather information that will be stored in HDFS. The information gathered can be log files, Twitter API, or website scraps. These data can be chained and subjected to analyses.","title":"Flume"},{"location":"big_data/#kafka","text":"Apache Kafka is an open-source stream-processing software. It aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Its storage layer is essentially a \"massively scalable pub/sub message queue designed as a distributed transaction log\".","title":"Kafka"},{"location":"big_data/#zookeeper","text":"Zookeeper is a centralized service that maintains, configures information, gives a name and provides distributed synchronization across a cluster. It imposes a file system-like hierarchy on the cluster and stores all of the metadata for the machines, so we can synchronize the work of the various machines.","title":"Zookeeper"},{"location":"dap/","text":"This process will help you understand, explore and use your data intelligently so that you make the most of the information you're given. Five steps: Question Wrangle Explore Draw conclusions Communicate. Question The data analysis process always starts with asking questions. Sometimes, you're already given a data set and glance over it to figure out good questions to ask. Other times, your questions come first, which will determine what kinds of data you'll gather later. In both cases, you should be thinking: what am I trying to find out? Is there a problem I'm trying to solve? Example: What are the characteristics of students who pass their projects? How can I better stock my store with products people want to buy? In the real world, you often deal with multiple sets of massive amounts of data, all in different forms. The right questions can really help you focus on relevant parts of your data and direct your analysis towards meaningful insights. Wrangle Once you have your questions, you'll need to wrangle your data to help you answer them. By that, I mean making sure you have all the data you need in great quality. There are three parts to this step: You gather your data. If you are already given that data, then all you need to do is open it, like importing it into a Jupyter notebook. If you weren't provided data, you need think carefully about what data would be most helpful in answering your questions and then collect them from all the sources available. You assess your data to identify any problems in your data's quality or structure. You clean your data. This often involves modifying, replacing, or moving data to ensure that your data set is as high quality and well-structured as possible. This wrangling step is all about getting the data you need in a form that you can work with. Explore Exploring involves finding patterns in your data, visualizing relationships in your data and just building intuition about what you're working with. After exploring, you can do things like remove outliers and create new and more descriptive features from existing data, also known as feature engineering . Many times modifying and engineer your data properly and even creatively can significantly increase the quality of your analysis. As you become more familiar with your data in this EDA step, you'll often revisit previous steps. Example : you might discover new problems in your data and go back to wrangle them. Or you might discover exciting, unexpected patterns and decide to refine your questions. The data analysis process isn't always linear. This exploratory step in particular is very intertwined with the rest of the process. It's usually where you discover and learn the most about your data. Conclusions After you've done your exploratory data analysis, you want to draw conclusions or even make predictions. Example : Predicting which students will fail a project so you can reach out to those students Or predicting which products are most likely to sell so you can start your store appropriately. Communicate Finally, you need to communicate your results to others. This is one of the most important skills you can develop. Your analysis is only as valuable as your ability to communicate it. You often need to justify and convey meaning in the insights you found Or if your end goal is to build a system, like a movie recommender or a news feed ranking algorithm, you usually share what you've built, explain how you reach design decisions and report how well it performs. You can communicate results in many ways: Reports Slide Decks Blog Posts Emails Presentations","title":"Data Analysis Process"},{"location":"dap/#question","text":"The data analysis process always starts with asking questions. Sometimes, you're already given a data set and glance over it to figure out good questions to ask. Other times, your questions come first, which will determine what kinds of data you'll gather later. In both cases, you should be thinking: what am I trying to find out? Is there a problem I'm trying to solve? Example: What are the characteristics of students who pass their projects? How can I better stock my store with products people want to buy? In the real world, you often deal with multiple sets of massive amounts of data, all in different forms. The right questions can really help you focus on relevant parts of your data and direct your analysis towards meaningful insights.","title":"Question"},{"location":"dap/#wrangle","text":"Once you have your questions, you'll need to wrangle your data to help you answer them. By that, I mean making sure you have all the data you need in great quality. There are three parts to this step: You gather your data. If you are already given that data, then all you need to do is open it, like importing it into a Jupyter notebook. If you weren't provided data, you need think carefully about what data would be most helpful in answering your questions and then collect them from all the sources available. You assess your data to identify any problems in your data's quality or structure. You clean your data. This often involves modifying, replacing, or moving data to ensure that your data set is as high quality and well-structured as possible. This wrangling step is all about getting the data you need in a form that you can work with.","title":"Wrangle"},{"location":"dap/#explore","text":"Exploring involves finding patterns in your data, visualizing relationships in your data and just building intuition about what you're working with. After exploring, you can do things like remove outliers and create new and more descriptive features from existing data, also known as feature engineering . Many times modifying and engineer your data properly and even creatively can significantly increase the quality of your analysis. As you become more familiar with your data in this EDA step, you'll often revisit previous steps. Example : you might discover new problems in your data and go back to wrangle them. Or you might discover exciting, unexpected patterns and decide to refine your questions. The data analysis process isn't always linear. This exploratory step in particular is very intertwined with the rest of the process. It's usually where you discover and learn the most about your data.","title":"Explore"},{"location":"dap/#conclusions","text":"After you've done your exploratory data analysis, you want to draw conclusions or even make predictions. Example : Predicting which students will fail a project so you can reach out to those students Or predicting which products are most likely to sell so you can start your store appropriately.","title":"Conclusions"},{"location":"dap/#communicate","text":"Finally, you need to communicate your results to others. This is one of the most important skills you can develop. Your analysis is only as valuable as your ability to communicate it. You often need to justify and convey meaning in the insights you found Or if your end goal is to build a system, like a movie recommender or a news feed ranking algorithm, you usually share what you've built, explain how you reach design decisions and report how well it performs. You can communicate results in many ways: Reports Slide Decks Blog Posts Emails Presentations","title":"Communicate"},{"location":"data-lakes/","text":"","title":"Data lakes"},{"location":"data_model/","text":"Data Model An abstraction that organizes elements of data and how they relate to each other. It can easily translate to database modeling, as this is the essential end state. It's also an iterative process. Data engineers continually reorganize, restructure, and optimize data models to fit the needs of the organization. Data Modeling process: Conceptual Logical Physical SQL Structured Query Language is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS). DDL DDL is short name of Data Definition Language, which deals with database schemas and descriptions, of how the data should reside in the database. CREATE : to create a database and its objects like (table, index, views, store procedure, function, and triggers) ALTER : alters the structure of the existing database DROP : delete objects from the database TRUNCATE : remove all records from a table, including all spaces allocated for the records are removed COMMENT : add comments to the data dictionary RENAME : rename an object DML DML is short name of Data Manipulation Language which deals with data manipulation and includes most common SQL statements such SELECT, INSERT, UPDATE, DELETE, etc., and it is used to store, modify, retrieve, delete and update data in a database. SELECT : retrieve data from a database INSERT : insert data into a table UPDATE : updates existing data within a table DELETE : Delete all records from a database table MERGE : UPSERT operation (insert or update) CALL : call a PL/SQL or Java subprogram EXPLAIN PLAN : interpretation of the data access path LOCK TABLE : concurrency Control DCL DCL is short name of Data Control Language which includes commands such as GRANT and mostly concerned with rights, permissions and other controls of the database system. GRANT : allow users access privileges to the database REVOKE : withdraw users access privileges given by using the GRANT command TCL TCL is short name of Transaction Control Language which deals with a transaction within a database. COMMIT : commits a Transaction ROLLBACK : rollback a transaction in case of any error occurs SAVEPOINT : to rollback the transaction making points within groups SET TRANSACTION : specify characteristics of the transaction Advantages of Using a Relational Database Flexibility for writing in SQL queries: With SQL being the most common database query language. Modeling the data not modeling queries Ability to do JOINS Ability to do aggregations and analytics Secondary Indexes available : You have the advantage of being able to add another index to help with quick searching. Smaller data volumes: If you have a smaller data volume (and not big data) you can use a relational database for its simplicity. ACID Transactions: Allows you to meet a set of properties of database transactions intended to guarantee validity even in the event of errors, power failures, and thus maintain data integrity. Easier to change to business requirements ACID Transactions Properties of database transactions intended to guarantee validity even in the event of errors or power failures. Atomicity : The whole transaction is processed or nothing is processed. A commonly cited example of an atomic transaction is money transactions between two bank accounts. The transaction of transferring money from one account to the other is made up of two operations. First, you have to withdraw money in one account, and second you have to save the withdrawn money to the second account. An atomic transaction, i.e., when either all operations occur or nothing occurs, keeps the database in a consistent state. This ensures that if either of those two operations (withdrawing money from the 1st account or saving the money to the 2nd account) fail, the money is neither lost nor created. Consistency : Only transactions that abide by constraints and rules are written into the database, otherwise the database keeps the previous state. The data should be correct across all rows and tables. Isolation : Transactions are processed independently and securely, order does not matter. A low level of isolation enables many users to access the data simultaneously, however this also increases the possibilities of concurrency effects (e.g., dirty reads or lost updates). On the other hand, a high level of isolation reduces these chances of concurrency effects, but also uses more system resources and transactions blocking each other. Durability : Completed transactions are saved to database even in cases of system failure. A commonly cited example includes tracking flight seat bookings. So once the flight booking records a confirmed seat booking, the seat remains booked even if a system failure occurs When Not to Use a Relational Database Have large amounts of data : Relational Databases are not distributed databases and because of this they can only scale vertically by adding more storage in the machine itself. You are limited by how much you can scale and how much data you can store on one machine. You cannot add more machines like you can in NoSQL databases. Need to be able to store different data type formats : Relational databases are not designed to handle unstructured data. Need high throughput -- fast reads: While ACID transactions bring benefits, they also slow down the process of reading and writing data. If you need very fast reads and writes, using a relational database may not suit your needs. Need a flexible schema : Flexible schema can allow for columns to be added that do not have to be used by every row, saving disk space. Need high availability : The fact that relational databases are not distributed (and even when they are, they have a coordinator/worker architecture), they have a single point of failure. When that database goes down, a fail-over to a backup system occurs and takes time. Need horizontal scalability : Horizontal scalability is the ability to add more machines or nodes to a system to increase performance and space for data. NoSQL NoSQL databases were created to do some of the issues faced with relational databases. Apache Cassandra (partition row store): The data is distributed by partitions across nodes or servers and the data is organized in the columns and rows format. MongoDB (document store): One of the defining characteristics of a document oriented database is that in addition to the key lookups performed by key-value store, the database also offers an API or query language that retrieves document based on its contents. It's basically easy to do search on documents. DynamoDB (key-value store): data is represented as a collection of key and value pairs. Apache HBase (wide column store): It also uses tables, rows and columns but unlike a relational database, the names and formats of the columns can vary from row to row in the same table. Neo4j (graph databas): it's all about relationships and the data is represented as nodes and edges. Apache Cassandra It provides scalability and high availability without compromising performace. Linear Scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data. What type of companies use Apache Cassandra? Uber uses Apache Cassandra for their entire backend. Netflix uses Apache Cassandra to serve all their videos to customers. Good use cases for NoSQL (and more specifically Apache Cassandra) are : Transaction logging (retail, health care) Internet of Things (IoT) Time series data Any workload that is heavy on writes to the database (since Apache Cassandra is optimized for writes). When to use a NoSQL Database Need to be able to store different data type formats : NoSQL was also created to handle different data configurations: structured, semi-structured, and unstructured data. JSON, XML documents can all be handled easily with NoSQL. Large amounts of data : Relational Databases are not distributed databases and because of this they can only scale vertically by adding more storage in the machine itself. NoSQL databases were created to be able to be horizontally scalable. The more servers/systems you add to the database the more data that can be hosted with high availability and low latency (fast reads and writes). Need horizontal scalability : Horizontal scalability is the ability to add more machines or nodes to a system to increase performance and space for data Need high throughput : While ACID transactions bring benefits they also slow down the process of reading and writing data. If you need very fast reads and writes using a relational database may not suit your needs. Need a flexible schema : Flexible schema can allow for columns to be added that do not have to be used by every row, saving disk space. Need high availability : Relational databases have a single point of failure. When that database goes down, a failover to a backup system must happen and takes time. When NOT to use a NoSQL Database? When you have a small dataset : NoSQL databases were made for big datasets not small datasets and while it works it wasn\u2019t created for that. When you need ACID Transactions : If you need a consistent database with ACID transactions, then most NoSQL databases will not be able to serve this need. NoSQL database are eventually consistent and do not provide ACID transactions. However, there are exceptions to it. Some non-relational databases like MongoDB can support ACID transactions. When you need the ability to do JOINS across tables : NoSQL does not allow the ability to do JOINS. This is not allowed as this will result in full table scans. If you want to be able to do aggregations and analytics If you have changing business requirements: Ad-hoc queries are possible but difficult as the data model was done to fix particular queries If your queries are not available and you need the flexibility : You need your queries in advance. If those are not available or you will need to be able to have flexibility on how you query your data you might need to stick with a relational database. Relational Data Model Importance of Relational Databases: Standardization of data model : Once your data is transformed into the rows and columns format, your data is standardized and you can query it with SQL Flexibility in adding and altering tables : Relational databases gives you flexibility to add tables, alter tables, add and remove data. Data Integrity : Data Integrity is the backbone of using a relational database. Standard Query Language (SQL) : A standard language can be used to access the data with a predefined language. Simplicity : Data is systematically stored and modeled in tabular format. Intuitive Organization : The spreadsheet format is intuitive but intuitive to data modeling in relational databases. Online Analytical Processing (OLAP) Databases optimized for these workloads allow for complex analytical and ad hoc queries, including aggregations. These type of databases are optimized for reads. Online Transactional Processing (OLTP) Databases optimized for these workloads allow for less complex queries in large volume. The types of queries for these databases are read, insert, update, and delete. The key to remember the difference between OLAP and OLTP is analytics (A) vs transactions (T). If you want to get the price of a shoe then you are using OLTP (this has very little or no aggregations). If you want to know the total stock of shoes a particular store sold, then this requires using OLAP (since this will require aggregations). Objectives of Normal Form To free the database from unwanted insertions, updates, & deletion dependencies To reduce the need for refactoring the database as new types of data are introduced To make the relational model more informative to users To make the database neutral to the query statistics How to reach First Normal Form (1NF): Atomic values: each cell contains unique and single values Be able to add data without altering tables Separate different relations into different tables Keep relationships between tables together with foreign keys Second Normal Form (2NF): Have reached 1NF All columns in the table must rely on the Primary Key Third Normal Form (3NF): Must be in 2nd Normal Form No transitive dependencies Remember, transitive dependencies you are trying to maintain is that to get from A-> C, you want to avoid going through B. Denormalization JOINS on the database allow for outstanding flexibility but are extremely slow. If you are dealing with heavy reads on your database, you may want to think about denormalizing your tables. You get your data into normalized form, and then you proceed with denormalization. So, denormalization comes after normalization. Fact and Dimension Table As you can see in the image, the unique primary key for each Dimension table is included in the Fact table. In this example, it helps to think about the Dimension tables providing the following information: Where the product was bought? (Dim_Store table) When the product was bought? (Dim_Date table) What product was bought? (Dim_Product table) The Fact table provides the metric of the business process (here Sales). How many units of products were bought? (Fact_Sales table) CAP Theorem A theorem in computer science that states it is impossible for a distributed data store to simulataneously provide more than two out of the following three guarantees of consisency, availability and partition tolerance. Consistency : Every read from the database gets the latest (and correct) piece of data or an error Availability : Every request is received and a response is given -- without a guarantee that the data is the latest update Partition Tolerance : The system continues to work regardless of losing network connectivity between nodes Data Modeling in Apache Cassandra: Denormalization is not just okay -- it's a must Denormalization must be done for fast reads Apache Cassandra has been optimized for fast writes ALWAYS think Queries first One table per query is a great strategy Apache Cassandra does not allow for JOINs between tables","title":"Data Model"},{"location":"data_model/#data-model","text":"An abstraction that organizes elements of data and how they relate to each other. It can easily translate to database modeling, as this is the essential end state. It's also an iterative process. Data engineers continually reorganize, restructure, and optimize data models to fit the needs of the organization. Data Modeling process: Conceptual Logical Physical","title":"Data Model"},{"location":"data_model/#sql","text":"Structured Query Language is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS).","title":"SQL"},{"location":"data_model/#ddl","text":"DDL is short name of Data Definition Language, which deals with database schemas and descriptions, of how the data should reside in the database. CREATE : to create a database and its objects like (table, index, views, store procedure, function, and triggers) ALTER : alters the structure of the existing database DROP : delete objects from the database TRUNCATE : remove all records from a table, including all spaces allocated for the records are removed COMMENT : add comments to the data dictionary RENAME : rename an object","title":"DDL"},{"location":"data_model/#dml","text":"DML is short name of Data Manipulation Language which deals with data manipulation and includes most common SQL statements such SELECT, INSERT, UPDATE, DELETE, etc., and it is used to store, modify, retrieve, delete and update data in a database. SELECT : retrieve data from a database INSERT : insert data into a table UPDATE : updates existing data within a table DELETE : Delete all records from a database table MERGE : UPSERT operation (insert or update) CALL : call a PL/SQL or Java subprogram EXPLAIN PLAN : interpretation of the data access path LOCK TABLE : concurrency Control","title":"DML"},{"location":"data_model/#dcl","text":"DCL is short name of Data Control Language which includes commands such as GRANT and mostly concerned with rights, permissions and other controls of the database system. GRANT : allow users access privileges to the database REVOKE : withdraw users access privileges given by using the GRANT command","title":"DCL"},{"location":"data_model/#tcl","text":"TCL is short name of Transaction Control Language which deals with a transaction within a database. COMMIT : commits a Transaction ROLLBACK : rollback a transaction in case of any error occurs SAVEPOINT : to rollback the transaction making points within groups SET TRANSACTION : specify characteristics of the transaction","title":"TCL"},{"location":"data_model/#advantages-of-using-a-relational-database","text":"Flexibility for writing in SQL queries: With SQL being the most common database query language. Modeling the data not modeling queries Ability to do JOINS Ability to do aggregations and analytics Secondary Indexes available : You have the advantage of being able to add another index to help with quick searching. Smaller data volumes: If you have a smaller data volume (and not big data) you can use a relational database for its simplicity. ACID Transactions: Allows you to meet a set of properties of database transactions intended to guarantee validity even in the event of errors, power failures, and thus maintain data integrity. Easier to change to business requirements","title":"Advantages of Using a Relational Database"},{"location":"data_model/#acid-transactions","text":"Properties of database transactions intended to guarantee validity even in the event of errors or power failures. Atomicity : The whole transaction is processed or nothing is processed. A commonly cited example of an atomic transaction is money transactions between two bank accounts. The transaction of transferring money from one account to the other is made up of two operations. First, you have to withdraw money in one account, and second you have to save the withdrawn money to the second account. An atomic transaction, i.e., when either all operations occur or nothing occurs, keeps the database in a consistent state. This ensures that if either of those two operations (withdrawing money from the 1st account or saving the money to the 2nd account) fail, the money is neither lost nor created. Consistency : Only transactions that abide by constraints and rules are written into the database, otherwise the database keeps the previous state. The data should be correct across all rows and tables. Isolation : Transactions are processed independently and securely, order does not matter. A low level of isolation enables many users to access the data simultaneously, however this also increases the possibilities of concurrency effects (e.g., dirty reads or lost updates). On the other hand, a high level of isolation reduces these chances of concurrency effects, but also uses more system resources and transactions blocking each other. Durability : Completed transactions are saved to database even in cases of system failure. A commonly cited example includes tracking flight seat bookings. So once the flight booking records a confirmed seat booking, the seat remains booked even if a system failure occurs","title":"ACID Transactions"},{"location":"data_model/#when-not-to-use-a-relational-database","text":"Have large amounts of data : Relational Databases are not distributed databases and because of this they can only scale vertically by adding more storage in the machine itself. You are limited by how much you can scale and how much data you can store on one machine. You cannot add more machines like you can in NoSQL databases. Need to be able to store different data type formats : Relational databases are not designed to handle unstructured data. Need high throughput -- fast reads: While ACID transactions bring benefits, they also slow down the process of reading and writing data. If you need very fast reads and writes, using a relational database may not suit your needs. Need a flexible schema : Flexible schema can allow for columns to be added that do not have to be used by every row, saving disk space. Need high availability : The fact that relational databases are not distributed (and even when they are, they have a coordinator/worker architecture), they have a single point of failure. When that database goes down, a fail-over to a backup system occurs and takes time. Need horizontal scalability : Horizontal scalability is the ability to add more machines or nodes to a system to increase performance and space for data.","title":"When Not to Use a Relational Database"},{"location":"data_model/#nosql","text":"NoSQL databases were created to do some of the issues faced with relational databases. Apache Cassandra (partition row store): The data is distributed by partitions across nodes or servers and the data is organized in the columns and rows format. MongoDB (document store): One of the defining characteristics of a document oriented database is that in addition to the key lookups performed by key-value store, the database also offers an API or query language that retrieves document based on its contents. It's basically easy to do search on documents. DynamoDB (key-value store): data is represented as a collection of key and value pairs. Apache HBase (wide column store): It also uses tables, rows and columns but unlike a relational database, the names and formats of the columns can vary from row to row in the same table. Neo4j (graph databas): it's all about relationships and the data is represented as nodes and edges.","title":"NoSQL"},{"location":"data_model/#apache-cassandra","text":"It provides scalability and high availability without compromising performace. Linear Scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data. What type of companies use Apache Cassandra? Uber uses Apache Cassandra for their entire backend. Netflix uses Apache Cassandra to serve all their videos to customers. Good use cases for NoSQL (and more specifically Apache Cassandra) are : Transaction logging (retail, health care) Internet of Things (IoT) Time series data Any workload that is heavy on writes to the database (since Apache Cassandra is optimized for writes).","title":"Apache Cassandra"},{"location":"data_model/#when-to-use-a-nosql-database","text":"Need to be able to store different data type formats : NoSQL was also created to handle different data configurations: structured, semi-structured, and unstructured data. JSON, XML documents can all be handled easily with NoSQL. Large amounts of data : Relational Databases are not distributed databases and because of this they can only scale vertically by adding more storage in the machine itself. NoSQL databases were created to be able to be horizontally scalable. The more servers/systems you add to the database the more data that can be hosted with high availability and low latency (fast reads and writes). Need horizontal scalability : Horizontal scalability is the ability to add more machines or nodes to a system to increase performance and space for data Need high throughput : While ACID transactions bring benefits they also slow down the process of reading and writing data. If you need very fast reads and writes using a relational database may not suit your needs. Need a flexible schema : Flexible schema can allow for columns to be added that do not have to be used by every row, saving disk space. Need high availability : Relational databases have a single point of failure. When that database goes down, a failover to a backup system must happen and takes time.","title":"When to use a NoSQL Database"},{"location":"data_model/#when-not-to-use-a-nosql-database","text":"When you have a small dataset : NoSQL databases were made for big datasets not small datasets and while it works it wasn\u2019t created for that. When you need ACID Transactions : If you need a consistent database with ACID transactions, then most NoSQL databases will not be able to serve this need. NoSQL database are eventually consistent and do not provide ACID transactions. However, there are exceptions to it. Some non-relational databases like MongoDB can support ACID transactions. When you need the ability to do JOINS across tables : NoSQL does not allow the ability to do JOINS. This is not allowed as this will result in full table scans. If you want to be able to do aggregations and analytics If you have changing business requirements: Ad-hoc queries are possible but difficult as the data model was done to fix particular queries If your queries are not available and you need the flexibility : You need your queries in advance. If those are not available or you will need to be able to have flexibility on how you query your data you might need to stick with a relational database.","title":"When NOT to use a NoSQL Database?"},{"location":"data_model/#relational-data-model","text":"Importance of Relational Databases: Standardization of data model : Once your data is transformed into the rows and columns format, your data is standardized and you can query it with SQL Flexibility in adding and altering tables : Relational databases gives you flexibility to add tables, alter tables, add and remove data. Data Integrity : Data Integrity is the backbone of using a relational database. Standard Query Language (SQL) : A standard language can be used to access the data with a predefined language. Simplicity : Data is systematically stored and modeled in tabular format. Intuitive Organization : The spreadsheet format is intuitive but intuitive to data modeling in relational databases.","title":"Relational Data Model"},{"location":"data_model/#online-analytical-processing-olap","text":"Databases optimized for these workloads allow for complex analytical and ad hoc queries, including aggregations. These type of databases are optimized for reads.","title":"Online Analytical Processing (OLAP)"},{"location":"data_model/#online-transactional-processing-oltp","text":"Databases optimized for these workloads allow for less complex queries in large volume. The types of queries for these databases are read, insert, update, and delete. The key to remember the difference between OLAP and OLTP is analytics (A) vs transactions (T). If you want to get the price of a shoe then you are using OLTP (this has very little or no aggregations). If you want to know the total stock of shoes a particular store sold, then this requires using OLAP (since this will require aggregations).","title":"Online Transactional Processing (OLTP)"},{"location":"data_model/#objectives-of-normal-form","text":"To free the database from unwanted insertions, updates, & deletion dependencies To reduce the need for refactoring the database as new types of data are introduced To make the relational model more informative to users To make the database neutral to the query statistics How to reach First Normal Form (1NF): Atomic values: each cell contains unique and single values Be able to add data without altering tables Separate different relations into different tables Keep relationships between tables together with foreign keys Second Normal Form (2NF): Have reached 1NF All columns in the table must rely on the Primary Key Third Normal Form (3NF): Must be in 2nd Normal Form No transitive dependencies Remember, transitive dependencies you are trying to maintain is that to get from A-> C, you want to avoid going through B.","title":"Objectives of Normal Form"},{"location":"data_model/#denormalization","text":"JOINS on the database allow for outstanding flexibility but are extremely slow. If you are dealing with heavy reads on your database, you may want to think about denormalizing your tables. You get your data into normalized form, and then you proceed with denormalization. So, denormalization comes after normalization.","title":"Denormalization"},{"location":"data_model/#fact-and-dimension-table","text":"As you can see in the image, the unique primary key for each Dimension table is included in the Fact table. In this example, it helps to think about the Dimension tables providing the following information: Where the product was bought? (Dim_Store table) When the product was bought? (Dim_Date table) What product was bought? (Dim_Product table) The Fact table provides the metric of the business process (here Sales). How many units of products were bought? (Fact_Sales table)","title":"Fact and Dimension Table"},{"location":"data_model/#cap-theorem","text":"A theorem in computer science that states it is impossible for a distributed data store to simulataneously provide more than two out of the following three guarantees of consisency, availability and partition tolerance. Consistency : Every read from the database gets the latest (and correct) piece of data or an error Availability : Every request is received and a response is given -- without a guarantee that the data is the latest update Partition Tolerance : The system continues to work regardless of losing network connectivity between nodes","title":"CAP Theorem"},{"location":"data_model/#data-modeling-in-apache-cassandra","text":"Denormalization is not just okay -- it's a must Denormalization must be done for fast reads Apache Cassandra has been optimized for fast writes ALWAYS think Queries first One table per query is a great strategy Apache Cassandra does not allow for JOINs between tables","title":"Data Modeling in Apache Cassandra:"},{"location":"data_quality/","text":"Data Lineage The data lineage of a dataset describes the discrete steps involved in the creation, movement, and calculation of that dataset. Why is Data Lineage important? Instilling Confidence: Being able to describe the data lineage of a particular dataset or analysis will build confidence in data consumers (engineers, analysts, data scientists, etc.) that our data pipeline is creating meaningful results using the correct datasets. If the data lineage is unclear, its less likely that the data consumers will trust or use the data. Defining Metrics: Another major benefit of surfacing data lineage is that it allows everyone in the organization to agree on the definition of how a particular metric is calculated. Debugging: Data lineage helps data engineers track down the root of errors when they occur. If each step of the data movement and transformation process is well described, it's easy to find problems when they occur. In general, data lineage has important implications for a business. Each department or business unit's success is tied to data and to the flow of data between departments. For e.g., sales departments rely on data to make sales forecasts, while at the same time the finance department would need to track sales and revenue. Each of these departments and roles depend on data, and knowing where to find the data. Data flow and data lineage tools enable data engineers and architects to track the flow of this large web of data. Schedules Pipelines are often driven by schedules which determine what data should be analyzed and when. Why Schedules? Pipeline schedules can reduce the amount of data that needs to be processed in a given run. It helps scope the job to only run the data for the time period since the data pipeline last ran. In a naive analysis, with no scope, we would analyze all of the data at all times. Using schedules to select only data relevant to the time period of the given pipeline execution can help improve the quality and accuracy of the analyses performed by our pipeline. Running pipelines on a schedule will decrease the time it takes the pipeline to run. An analysis of larger scope can leverage already-completed work. For. e.g., if the aggregates for all months prior to now have already been done by a scheduled job, then we only need to perform the aggregation for the current month and add it to the existing totals. Selecting the time period Determining the appropriate time period for a schedule is based on a number of factors which you need to consider as the pipeline designer. What is the size of data, on average, for a time period? If an entire years worth of data is only a few kb or mb, then perhaps its fine to load the entire dataset. If an hours worth of data is hundreds of mb or even in the gbs then likely you will need to schedule your pipeline more frequently. How frequently is data arriving, and how often does the analysis need to be performed? If our bikeshare company needs trip data every hour, that will be a driving factor in determining the schedule. Alternatively, if we have to load hundreds of thousands of tiny records, even if they don't add up to much in terms of mb or gb, the file access alone will slow down our analysis and we\u2019ll likely want to run it more often. What's the frequency on related datasets? A good rule of thumb is that the frequency of a pipeline\u2019s schedule should be determined by the dataset in our pipeline which requires the most frequent analysis. This isn\u2019t universally the case, but it's a good starting assumption. For example, if our trips data is updating every hour, but our bikeshare station table only updates once a quarter, we\u2019ll probably want to run our trip analysis every hour, and not once a quarter. Data Partitioning Schedule partitioning Not only are schedules great for reducing the amount of data our pipelines have to process, but they also help us guarantee that we can meet timing guarantees that our data consumers may need. Logical partitioning Conceptually related data can be partitioned into discrete segments and processed separately. This process of separating data based on its conceptual relationship is called logical partitioning. With logical partitioning, unrelated things belong in separate steps. Consider your dependencies and separate processing around those boundaries. Also worth mentioning, the data location is another form of logical partitioning. For example, if our data is stored in a key-value store like Amazon's S3 in a format such as: s3://<bucket>/<year>/<month>/<day> we could say that our date is logically partitioned by time. Size Partitioning Size partitioning separates data for processing based on desired or required storage limits. This essentially sets the amount of data included in a data pipeline run. Size partitioning is critical to understand when working with large datasets, especially with Airflow. Examples of Data Quality Requirements: Data must be a certain size Data must be accurate to some margin of error Data must arrive within a given timeframe from the start of execution Pipelines must run on a particular schedule Data must not contain any sensitive information","title":"Data Quality"},{"location":"data_quality/#data-lineage","text":"The data lineage of a dataset describes the discrete steps involved in the creation, movement, and calculation of that dataset. Why is Data Lineage important? Instilling Confidence: Being able to describe the data lineage of a particular dataset or analysis will build confidence in data consumers (engineers, analysts, data scientists, etc.) that our data pipeline is creating meaningful results using the correct datasets. If the data lineage is unclear, its less likely that the data consumers will trust or use the data. Defining Metrics: Another major benefit of surfacing data lineage is that it allows everyone in the organization to agree on the definition of how a particular metric is calculated. Debugging: Data lineage helps data engineers track down the root of errors when they occur. If each step of the data movement and transformation process is well described, it's easy to find problems when they occur. In general, data lineage has important implications for a business. Each department or business unit's success is tied to data and to the flow of data between departments. For e.g., sales departments rely on data to make sales forecasts, while at the same time the finance department would need to track sales and revenue. Each of these departments and roles depend on data, and knowing where to find the data. Data flow and data lineage tools enable data engineers and architects to track the flow of this large web of data.","title":"Data Lineage"},{"location":"data_quality/#schedules","text":"Pipelines are often driven by schedules which determine what data should be analyzed and when. Why Schedules? Pipeline schedules can reduce the amount of data that needs to be processed in a given run. It helps scope the job to only run the data for the time period since the data pipeline last ran. In a naive analysis, with no scope, we would analyze all of the data at all times. Using schedules to select only data relevant to the time period of the given pipeline execution can help improve the quality and accuracy of the analyses performed by our pipeline. Running pipelines on a schedule will decrease the time it takes the pipeline to run. An analysis of larger scope can leverage already-completed work. For. e.g., if the aggregates for all months prior to now have already been done by a scheduled job, then we only need to perform the aggregation for the current month and add it to the existing totals.","title":"Schedules"},{"location":"data_quality/#selecting-the-time-period","text":"Determining the appropriate time period for a schedule is based on a number of factors which you need to consider as the pipeline designer. What is the size of data, on average, for a time period? If an entire years worth of data is only a few kb or mb, then perhaps its fine to load the entire dataset. If an hours worth of data is hundreds of mb or even in the gbs then likely you will need to schedule your pipeline more frequently. How frequently is data arriving, and how often does the analysis need to be performed? If our bikeshare company needs trip data every hour, that will be a driving factor in determining the schedule. Alternatively, if we have to load hundreds of thousands of tiny records, even if they don't add up to much in terms of mb or gb, the file access alone will slow down our analysis and we\u2019ll likely want to run it more often. What's the frequency on related datasets? A good rule of thumb is that the frequency of a pipeline\u2019s schedule should be determined by the dataset in our pipeline which requires the most frequent analysis. This isn\u2019t universally the case, but it's a good starting assumption. For example, if our trips data is updating every hour, but our bikeshare station table only updates once a quarter, we\u2019ll probably want to run our trip analysis every hour, and not once a quarter.","title":"Selecting the time period"},{"location":"data_quality/#data-partitioning","text":"","title":"Data Partitioning"},{"location":"data_quality/#schedule-partitioning","text":"Not only are schedules great for reducing the amount of data our pipelines have to process, but they also help us guarantee that we can meet timing guarantees that our data consumers may need.","title":"Schedule partitioning"},{"location":"data_quality/#logical-partitioning","text":"Conceptually related data can be partitioned into discrete segments and processed separately. This process of separating data based on its conceptual relationship is called logical partitioning. With logical partitioning, unrelated things belong in separate steps. Consider your dependencies and separate processing around those boundaries. Also worth mentioning, the data location is another form of logical partitioning. For example, if our data is stored in a key-value store like Amazon's S3 in a format such as: s3://<bucket>/<year>/<month>/<day> we could say that our date is logically partitioned by time.","title":"Logical partitioning"},{"location":"data_quality/#size-partitioning","text":"Size partitioning separates data for processing based on desired or required storage limits. This essentially sets the amount of data included in a data pipeline run. Size partitioning is critical to understand when working with large datasets, especially with Airflow.","title":"Size Partitioning"},{"location":"data_quality/#examples-of-data-quality-requirements","text":"Data must be a certain size Data must be accurate to some margin of error Data must arrive within a given timeframe from the start of execution Pipelines must run on a particular schedule Data must not contain any sensitive information","title":"Examples of Data Quality Requirements:"},{"location":"data_science/","text":"Questions Data Science Answers Is it A or B? : Classfication algorithms whether a coupon or discount bring more customer. Is this weird? : Anomaly detection algorithms credit card spending habit, flags unsual behaviour. How much? How many? : Regression algorithms what will my nth quarter sales be. How is this organized? : Clustering algorithms which viewers like same type of movies, which printer model fails same way. What should I do now? : Reinforcement Learning algorithms self driving car, whether to brake or accelerate at yellow light. Is data ready for Data Science Relevant? Connected? Accurate? Enough to work with? New model building workflow Gather data Feature extraction Pick ML framework Spin up AWS EC2 instance Setup machine Launch training job Analyze results When you have a standard computer, and you want to run multiple processes on it, the operating system is what deals with the scheduling - it determines which process gets access to which processor at a given point in time, which process gets access to which parts of memory, decides which processes should be frozen or die in cases where there is a shortage of resources. Use Docker to containerize model development environment, setup code and dependencies on each remote machine. Kubernetes is an open-source container-orchestration system for automating application deployment, scaling and management. These tools allow for easy to use debugging and monitoring. Borg is a Cluster OS Google uses for managing internal workloads. It manages thousands of machines with very good internal network connectivity. When you want to run a \u201cservice\u201d - which is the equivalent of a \u201cdaemon\u201d on a normal machine, something that should stay up all the time - you \u201crun it on Borg\u201d - that is, you tell the Borg cluster scheduler that you want, say, 200 copies of the binary that determines the service. The Borg scheduler identifies machines in the cluster that will have the spare capacity to run your service and sends a request to run the service to the node agents on these machines. Descriptive Statistics Descriptive statistics summarize a given data set, which can be either a representation of the entire or a sample of a population. It is broken down into measure of central tendency (mean, median & mode) and measure of variability (standard deviation, variance & skewness). Inferential Statistics It is the process of using data analysis to deduce properties of an underlying probability distribution. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and driving estimates. It is assumed that the observed data set is sampled from a larger population.","title":"Data Science"},{"location":"data_science/#questions-data-science-answers","text":"Is it A or B? : Classfication algorithms whether a coupon or discount bring more customer. Is this weird? : Anomaly detection algorithms credit card spending habit, flags unsual behaviour. How much? How many? : Regression algorithms what will my nth quarter sales be. How is this organized? : Clustering algorithms which viewers like same type of movies, which printer model fails same way. What should I do now? : Reinforcement Learning algorithms self driving car, whether to brake or accelerate at yellow light.","title":"Questions Data Science Answers"},{"location":"data_science/#is-data-ready-for-data-science","text":"Relevant? Connected? Accurate? Enough to work with?","title":"Is data ready for Data Science"},{"location":"data_science/#new-model-building-workflow","text":"Gather data Feature extraction Pick ML framework Spin up AWS EC2 instance Setup machine Launch training job Analyze results When you have a standard computer, and you want to run multiple processes on it, the operating system is what deals with the scheduling - it determines which process gets access to which processor at a given point in time, which process gets access to which parts of memory, decides which processes should be frozen or die in cases where there is a shortage of resources. Use Docker to containerize model development environment, setup code and dependencies on each remote machine. Kubernetes is an open-source container-orchestration system for automating application deployment, scaling and management. These tools allow for easy to use debugging and monitoring. Borg is a Cluster OS Google uses for managing internal workloads. It manages thousands of machines with very good internal network connectivity. When you want to run a \u201cservice\u201d - which is the equivalent of a \u201cdaemon\u201d on a normal machine, something that should stay up all the time - you \u201crun it on Borg\u201d - that is, you tell the Borg cluster scheduler that you want, say, 200 copies of the binary that determines the service. The Borg scheduler identifies machines in the cluster that will have the spare capacity to run your service and sends a request to run the service to the node agents on these machines.","title":"New model building workflow"},{"location":"data_science/#descriptive-statistics","text":"Descriptive statistics summarize a given data set, which can be either a representation of the entire or a sample of a population. It is broken down into measure of central tendency (mean, median & mode) and measure of variability (standard deviation, variance & skewness).","title":"Descriptive Statistics"},{"location":"data_science/#inferential-statistics","text":"It is the process of using data analysis to deduce properties of an underlying probability distribution. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and driving estimates. It is assumed that the observed data set is sampled from a larger population.","title":"Inferential Statistics"},{"location":"data_warehouse/","text":"What Is A Data Warehouse? It is a system (including processes, technologies & data representations) that enables us to support analytical processes. A Business Perspective You are in charge of a retailer\u2019s data infrastructure. Let\u2019s look at some business activities. Customers should be able to find goods & make orders Inventory Staff should be able to stock, retrieve, and re-order goods Delivery Staff should be able to pick up & deliver goods HR should be able to assess the performance of sales staff Marketing should be able to see the effect of different sales channels Management should be able to monitor sales growth Ask yourself: Can I build a database to support these activities? Are all of the above questions of the same nature? Let's take a closer look at details that may affect your data infrastructure. Retailer has a nation-wide presence \u2192 Scale? Acquired smaller retailers, brick & mortar shops, online store \u2192 Single database? Complexity? Has support call center & social media accounts \u2192 Tabular data? Customers, Inventory Staff and Delivery staff expect the system to be fast & stable \u2192 Performance HR, Marketing & Sales Reports want a lot information but have not decided yet on everything they need \u2192 Clear Requirements? A Technical Perspective A data warehouse is a copy of transaction data specifically structured for query and analysis. A data warehouse is a subject-oriented, integrated, nonvolatile and time-variant collection of data in support of management's decisions. A data warehouse is a system that retrieves and consolidates data periodically from source systems into a dimensional or normalized data store. It usually keeps years of history and is queried for business intelligence or other analytical activities. It is typically updated in batches, not every time a transaction happens in the source system. Data Warehouse Goals Simple to understand Performant Quality Assured Handles new questions well Secure Facts & Dimenstions If you are unsure if a column is a fact or dimension, the simplest rule is that a fact is usually: Numeric & Additive Fact tables: Record business events, like an order, a phone call, a book review Fact tables columns record events recorded in quantifiable metrics like quantity of an item, duration of a call, a book rating. Dimension tables: Record the context of the business events, e.g: who, what, where, why etc. Dimension tables columns contain attributes like the store at which an item is purchased, or the customer who make the call, etc.","title":"Data Warehouse"},{"location":"data_warehouse/#what-is-a-data-warehouse","text":"It is a system (including processes, technologies & data representations) that enables us to support analytical processes.","title":"What Is A Data Warehouse?"},{"location":"data_warehouse/#a-business-perspective","text":"You are in charge of a retailer\u2019s data infrastructure. Let\u2019s look at some business activities. Customers should be able to find goods & make orders Inventory Staff should be able to stock, retrieve, and re-order goods Delivery Staff should be able to pick up & deliver goods HR should be able to assess the performance of sales staff Marketing should be able to see the effect of different sales channels Management should be able to monitor sales growth Ask yourself: Can I build a database to support these activities? Are all of the above questions of the same nature? Let's take a closer look at details that may affect your data infrastructure. Retailer has a nation-wide presence \u2192 Scale? Acquired smaller retailers, brick & mortar shops, online store \u2192 Single database? Complexity? Has support call center & social media accounts \u2192 Tabular data? Customers, Inventory Staff and Delivery staff expect the system to be fast & stable \u2192 Performance HR, Marketing & Sales Reports want a lot information but have not decided yet on everything they need \u2192 Clear Requirements?","title":"A Business Perspective"},{"location":"data_warehouse/#a-technical-perspective","text":"A data warehouse is a copy of transaction data specifically structured for query and analysis. A data warehouse is a subject-oriented, integrated, nonvolatile and time-variant collection of data in support of management's decisions. A data warehouse is a system that retrieves and consolidates data periodically from source systems into a dimensional or normalized data store. It usually keeps years of history and is queried for business intelligence or other analytical activities. It is typically updated in batches, not every time a transaction happens in the source system.","title":"A Technical Perspective"},{"location":"data_warehouse/#data-warehouse-goals","text":"Simple to understand Performant Quality Assured Handles new questions well Secure","title":"Data Warehouse Goals"},{"location":"data_warehouse/#facts-dimenstions","text":"If you are unsure if a column is a fact or dimension, the simplest rule is that a fact is usually: Numeric & Additive Fact tables: Record business events, like an order, a phone call, a book review Fact tables columns record events recorded in quantifiable metrics like quantity of an item, duration of a call, a book rating. Dimension tables: Record the context of the business events, e.g: who, what, where, why etc. Dimension tables columns contain attributes like the store at which an item is purchased, or the customer who make the call, etc.","title":"Facts &amp; Dimenstions"},{"location":"info/","text":"CPU (Central Processing Unit) The CPU is the \"brain\" of the computer. Every process on your computer is eventually handled by your CPU. This includes calculations and also instructions for the other components of the compute. Memory (RAM) When your program runs, data gets temporarily stored in memory before getting sent to the CPU. Memory is ephemeral storage - when your computer shuts down, the data in the memory is lost. Storage (SSD or Magnetic Disk) Storage is used for keeping data over long periods of time. When a program runs, the CPU will direct the memory to temporarily load data from long-term storage. Network (LAN or the Internet) Network is the gateway for anything that you need that isn't stored on your computer. The network could connect to other computers in the same room (a Local Area Network) or to a computer on the other side of the world, connected over the internet. Limitations If a dataset is larger than the size of your RAM, you might still be able to analyze the data on a single computer. By default, the Python pandas library will read in an entire dataset from disk into memory. If the dataset is larger than your computer's memory, the program won't work. However, the Python pandas library can read in a file in smaller chunks. Thus, if you were going to calculate summary statistics about the dataset such as a sum or count, you could read in a part of the dataset at a time and accumulate the sum or count. Streaming Data Data streaming is a specialized topic in big data. The use case is when you want to store and analyze data in real-time such as Facebook posts or Twitter tweets. Hadoop vs Spark Spark contains libraries for data analysis, machine learning, graph analysis, and streaming live data. Spark is generally faster than Hadoop. This is because Hadoop writes intermediate results to disk whereas Spark tries to keep intermediate results in memory whenever possible. The Hadoop ecosystem includes a distributed file storage system called HDFS (Hadoop Distributed File System). Spark, on the other hand, does not include a file storage system. You can use Spark on top of HDFS but you do not have to. Spark can read in data from other sources as well such as Amazon S3. Spark's Limitations Spark Streaming\u2019s latency is at least 500 milliseconds since it operates on micro-batches of records, instead of processing one record at a time. Native streaming tools such as Storm, Apex, or Flink can push down this latency value and might be more suitable for low-latency applications. Flink and Apex can be used for batch computation as well, so if you're already using them for stream processing, there's no need to add Spark to your stack of technologies. Another limitation of Spark is its selection of machine learning algorithms. Currently, Spark only supports algorithms that scale linearly with the input data size. In general, deep learning is not available either, though there are many projects integrate Spark with Tensorflow and other deep learning tools. Beyond Spark for Storing and Processing Big Data Keep in mind that Spark is not a data storage system, and there are a number of tools besides Spark that can be used to process and analyze large datasets. Sometimes it makes sense to use the power and simplicity of SQL on big data. For these cases, a new class of databases, know as NoSQL and NewSQL, have been developed. For example, you might hear about newer database storage systems like HBase or Cassandra. There are also distributed SQL engines like Impala and Presto. Many of these technologies use query syntax that you are likely already familiar with based on your experiences with Python and SQL. Task Boundaries DAG tasks should be designed such that they are: Atomic and have a single purpose Maximize parallelism Make failure states obvious Every task in your dag should perform only one job. \u201cWrite programs that do one thing and do it well.\u201d - Ken Thompson\u2019s Unix Philosophy Benefits of Task Boundaries Re-visitable: Task boundaries are useful for you if you revisit a pipeline you wrote after a 6 month absence. You'll have a much easier time understanding how it works and the lineage of the data if the boundaries between tasks are clear and well defined. This is true in the code itself, and within the Airflow UI. Tasks that do just one thing are often more easily parallelized. This parallelization can offer a significant speedup in the execution of our DAGs.","title":"Info"},{"location":"info/#cpu-central-processing-unit","text":"The CPU is the \"brain\" of the computer. Every process on your computer is eventually handled by your CPU. This includes calculations and also instructions for the other components of the compute.","title":"CPU (Central Processing Unit)"},{"location":"info/#memory-ram","text":"When your program runs, data gets temporarily stored in memory before getting sent to the CPU. Memory is ephemeral storage - when your computer shuts down, the data in the memory is lost.","title":"Memory (RAM)"},{"location":"info/#storage-ssd-or-magnetic-disk","text":"Storage is used for keeping data over long periods of time. When a program runs, the CPU will direct the memory to temporarily load data from long-term storage.","title":"Storage (SSD or Magnetic Disk)"},{"location":"info/#network-lan-or-the-internet","text":"Network is the gateway for anything that you need that isn't stored on your computer. The network could connect to other computers in the same room (a Local Area Network) or to a computer on the other side of the world, connected over the internet.","title":"Network (LAN or the Internet)"},{"location":"info/#limitations","text":"If a dataset is larger than the size of your RAM, you might still be able to analyze the data on a single computer. By default, the Python pandas library will read in an entire dataset from disk into memory. If the dataset is larger than your computer's memory, the program won't work. However, the Python pandas library can read in a file in smaller chunks. Thus, if you were going to calculate summary statistics about the dataset such as a sum or count, you could read in a part of the dataset at a time and accumulate the sum or count.","title":"Limitations"},{"location":"info/#streaming-data","text":"Data streaming is a specialized topic in big data. The use case is when you want to store and analyze data in real-time such as Facebook posts or Twitter tweets.","title":"Streaming Data"},{"location":"info/#hadoop-vs-spark","text":"Spark contains libraries for data analysis, machine learning, graph analysis, and streaming live data. Spark is generally faster than Hadoop. This is because Hadoop writes intermediate results to disk whereas Spark tries to keep intermediate results in memory whenever possible. The Hadoop ecosystem includes a distributed file storage system called HDFS (Hadoop Distributed File System). Spark, on the other hand, does not include a file storage system. You can use Spark on top of HDFS but you do not have to. Spark can read in data from other sources as well such as Amazon S3.","title":"Hadoop vs Spark"},{"location":"info/#sparks-limitations","text":"Spark Streaming\u2019s latency is at least 500 milliseconds since it operates on micro-batches of records, instead of processing one record at a time. Native streaming tools such as Storm, Apex, or Flink can push down this latency value and might be more suitable for low-latency applications. Flink and Apex can be used for batch computation as well, so if you're already using them for stream processing, there's no need to add Spark to your stack of technologies. Another limitation of Spark is its selection of machine learning algorithms. Currently, Spark only supports algorithms that scale linearly with the input data size. In general, deep learning is not available either, though there are many projects integrate Spark with Tensorflow and other deep learning tools.","title":"Spark's Limitations"},{"location":"info/#beyond-spark-for-storing-and-processing-big-data","text":"Keep in mind that Spark is not a data storage system, and there are a number of tools besides Spark that can be used to process and analyze large datasets. Sometimes it makes sense to use the power and simplicity of SQL on big data. For these cases, a new class of databases, know as NoSQL and NewSQL, have been developed. For example, you might hear about newer database storage systems like HBase or Cassandra. There are also distributed SQL engines like Impala and Presto. Many of these technologies use query syntax that you are likely already familiar with based on your experiences with Python and SQL.","title":"Beyond Spark for Storing and Processing Big Data"},{"location":"info/#task-boundaries","text":"DAG tasks should be designed such that they are: Atomic and have a single purpose Maximize parallelism Make failure states obvious Every task in your dag should perform only one job. \u201cWrite programs that do one thing and do it well.\u201d - Ken Thompson\u2019s Unix Philosophy","title":"Task Boundaries"},{"location":"info/#benefits-of-task-boundaries","text":"Re-visitable: Task boundaries are useful for you if you revisit a pipeline you wrote after a 6 month absence. You'll have a much easier time understanding how it works and the lineage of the data if the boundaries between tasks are clear and well defined. This is true in the code itself, and within the Airflow UI. Tasks that do just one thing are often more easily parallelized. This parallelization can offer a significant speedup in the execution of our DAGs.","title":"Benefits of Task Boundaries"},{"location":"libraries/","text":"","title":"Libraries"},{"location":"terminology/","text":"Airflow Hooks: Hooks provide a reusable interface to external systems and databases. With hooks, you don\u2019t have to worry about how and where to store the connection strings and secrets in your code. Data Lineage: The data lineage of a dataset describes the discrete steps involved in the creation, movement, and calculation of that dataset. Data Validation: Data Validation is the process of ensuring that data is present, correct & meaningful. Ensuring the quality of your data through automated validation checks is a critical step in building data pipelines at any organization. Database ( Components of Airflow): Saves credentials, connections, history, and configuration Directed Acyclic Graphs (DAGs): DAGs are a special subset of graphs in which the edges between nodes have a specific direction, and no cycles exist. When we say \u201cno cycles exist\u201d what we mean is the nodes cant create a path back to themselves. Edges: The dependencies or relationships other between nodes. End Date: Airflow pipelines can also have end dates. You can use an end_date with your pipeline to let Airflow know when to stop running the pipeline. End_dates can also be useful when you want to perform an overhaul or redesign of an existing pipeline. Update the old pipeline with an end_date and then have the new pipeline start on the end date of the old pipeline. Logical partitioning: Conceptually related data can be partitioned into discrete segments and processed separately. This process of separating data based on its conceptual relationship is called logical partitioning. With logical partitioning, unrelated things belong in separate steps. Consider your dependencies and separate processing around those boundaries. Nodes: A step in the data pipeline process. Operators: Operators define the atomic steps of work that make up a DAG. Airflow comes with many Operators that can perform common operations. Schedule partitioning: Not only are schedules great for reducing the amount of data our pipelines have to process, but they also help us guarantee that we can meet timing guarantees that our data consumers may need. Scheduler ( Components of Airflow:) orchestrates the execution of jobs on a trigger or schedule Schedules: Data pipelines are often driven by schedules which determine what data should be analyzed and when. Size Partitioning: Size partitioning separates data for processing based on desired or required storage limits. This essentially sets the amount of data included in a data pipeline run. Size partitioning is critical to understand when working with large datasets, especially with Airflow. Start Date: Airflow will begin running pipelines on the start date selected. Whenever the start date of a DAG is in the past, and the time difference between the start date and now includes more than one schedule intervals, Airflow will automatically schedule and execute a DAG run to satisfy each one of those intervals. Task Dependencies: Describe the tasks or nodes in the Airflow DAGs in the order in which they should execute Web Interface ( Components of Airflow): provides a control dashboard for users and maintainers Work Queue ( Components of Airflow): holds the state of running DAGS and Tasks Worker processes ( Components of Airflow): that execute the operations defined in each DAG","title":"Terminology"}]}